{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faca5110",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assigment 5\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* last lab before 27.06.2022 \n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )\n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Consider the vowel reconstruction task -- i.e. inserting missing vowels (aeuioy) to obtain proper English text. For instance for the input sentence:\n",
    "\n",
    "<pre>\n",
    "h m gd smbd hs stln ll m vwls\n",
    "</pre>\n",
    "\n",
    "the best result is\n",
    "\n",
    "<pre>\n",
    "oh my god somebody has stolen all my vowels\n",
    "</pre>\n",
    "\n",
    "In this task both dev and test data come from the two books about Winnie-the-Pooh. You have to train two RNN Language Models on *pooh-train.txt*. For the first model use the code below, for the second choose different hyperparameters (different dropout, smaller number of units or layers, or just do any modification you want). \n",
    "\n",
    "The code below is based on\n",
    "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d50e03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEQUENCE_LENGTH = 15\n",
    "\n",
    "class PoohDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequence_length, device):\n",
    "        txt = open('pooh_train.txt').read()\n",
    "        \n",
    "        self.words = txt.lower().split() # The text is already tokenized\n",
    "        \n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
    "        )\n",
    "        \n",
    "pooh_dataset = PoohDataset(SEQUENCE_LENGTH, device)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b22a87b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LSTMModel(\n  (embedding): Embedding(2548, 100)\n  (lstm): LSTM(100, 512, num_layers=2, dropout=0.2)\n  (fc): Linear(in_features=512, out_features=2548, bias=True)\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, dataset, device):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.embedding_dim = 100\n",
    "        self.num_layers = 2\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
    "        \n",
    "model = LSTMModel(pooh_dataset, device) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074d5f7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 512\n",
    "max_epochs = 30\n",
    "\n",
    "def train(dataset, model):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.init_state(SEQUENCE_LENGTH)\n",
    "        \n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(pooh_dataset, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad0a59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pooh_2x512_30ep.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('pooh_2x512_30ep.model', map_location=torch.device('cpu')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1eb733",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863\n"
     ]
    }
   ],
   "source": [
    "# You can use the code if you want\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "vowels = set(\"aoiuye'\")\n",
    "def devowelize(s):\n",
    "    rv = ''.join(a for a in s if a not in vowels)\n",
    "    if rv:\n",
    "        return rv\n",
    "    return '_' # Symbol for words without consonants\n",
    "\n",
    "pooh_words = set(open('pooh_words.txt').read().split())\n",
    "representation = dd(set)\n",
    "\n",
    "for w in pooh_words:\n",
    "    r = devowelize(w)\n",
    "    representation[r].add(w)\n",
    "    \n",
    "hard_words = set()\n",
    "for r, ws in representation.items():\n",
    "    if len(ws) > 1:\n",
    "        hard_words.update(ws)\n",
    "        \n",
    "print (len(hard_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_matching_logits(logits, word, dataset):\n",
    "    keys = list(representation[word])\n",
    "    values = []\n",
    "    for reconstructed_word in keys:\n",
    "        if reconstructed_word in dataset.word_to_index:\n",
    "            values.append(logits[dataset.word_to_index[reconstructed_word]].item())\n",
    "        else:\n",
    "            values.append(min(logits))\n",
    "    return keys, values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "original_text = open('pooh_test.txt').read().lower().split()\n",
    "devowelized_text = [devowelize(w) for w in original_text]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# the predict function is a text generator. you have to modify this code!\n",
    "\n",
    "def predict(dataset, model, words, next_words, prev_range=5, temperature=1):\n",
    "    model.eval()\n",
    "\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    probability = 0\n",
    "\n",
    "    previous_indices = []\n",
    "    for word in words:\n",
    "        if word in dataset.word_to_index:\n",
    "            previous_indices.append(dataset.word_to_index[word])\n",
    "\n",
    "    for i in range(0, len(next_words)):\n",
    "        x = torch.tensor([previous_indices[-prev_range:]])\n",
    "        x = x.to(device)\n",
    "\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        matching_words, matching_logits = get_matching_logits(last_word_logits, next_words[i], dataset)\n",
    "        p = nn.functional.softmax(torch.tensor(matching_logits)/temperature, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(matching_logits), p=p)\n",
    "        chosen_word = matching_words[word_index]\n",
    "        words.append(chosen_word)\n",
    "        if chosen_word in dataset.word_to_index:\n",
    "            previous_indices.append(dataset.word_to_index[chosen_word])\n",
    "\n",
    "        probability += np.log(p[word_index])\n",
    "\n",
    "    return ' '.join(words), probability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def accuracy(original_sequence, reconstructed_sequence):\n",
    "    sa = original_sequence\n",
    "    sb = reconstructed_sequence\n",
    "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
    "    return score / len(original_sequence)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "random_prediction = []\n",
    "for word in devowelized_text:\n",
    "    random_prediction.append(np.random.choice(list(representation[word])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5365480290853425"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(original_text, random_prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "prev_range = 3\n",
    "prompt = original_text[:prev_range]\n",
    "next_words = devowelized_text[prev_range:]\n",
    "prediction, _ = predict(pooh_dataset, model, prompt, next_words, prev_range=prev_range)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8413062890674831"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(original_text, prediction.split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_prediction = None\n",
    "best_log_prob = -np.inf\n",
    "\n",
    "prev_range = 3\n",
    "for _ in range(10):\n",
    "    prompt = original_text[:prev_range]\n",
    "    next_words = devowelized_text[prev_range:]\n",
    "    prediction, log_prob = predict(pooh_dataset, model, prompt, next_words, prev_range=prev_range)\n",
    "    if log_prob > best_log_prob:\n",
    "        best_prediction = prediction\n",
    "        best_log_prob = log_prob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8423268274014543"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(original_text, best_prediction.split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "LSTMModel(\n  (embedding): Embedding(2548, 50)\n  (lstm): LSTM(50, 256, num_layers=2, dropout=0.2)\n  (fc): Linear(in_features=256, out_features=2548, bias=True)\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, dataset, device):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm_size = 256\n",
    "        self.embedding_dim = 50\n",
    "        self.num_layers = 2\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
    "\n",
    "model = LSTMModel(pooh_dataset, device)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('pooh_2x256_30ep.model', map_location=torch.device('cpu')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "prev_range = 3\n",
    "prompt = original_text[:prev_range]\n",
    "next_words = devowelized_text[prev_range:]\n",
    "prediction, _ = predict(pooh_dataset, model, prompt, next_words, prev_range=prev_range)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8405408853170048"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(original_text, prediction.split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "best_prediction = None\n",
    "best_log_prob = -np.inf\n",
    "\n",
    "prev_range = 3\n",
    "for _ in range(10):\n",
    "    prompt = original_text[:prev_range]\n",
    "    next_words = devowelized_text[prev_range:]\n",
    "    prediction, log_prob = predict(pooh_dataset, model, prompt, next_words, prev_range=prev_range)\n",
    "    if log_prob > best_log_prob:\n",
    "        best_prediction = prediction\n",
    "        best_log_prob = log_prob\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "0.840285750733512"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(original_text, best_prediction.split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "273881ae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can assume that only words from pooh_words.txt can occur in the reconstructed text. For decoding you have two options (choose one, or implement both ang get **+1** bonus point)\n",
    "\n",
    "1. Sample reconstructed text several times (with quite a low temperature), choose the most likely result.\n",
    "2. Perform beam search.\n",
    "\n",
    "Of course in the sampling procedure you should consider only words matching the given consonants.\n",
    "\n",
    "Report accuracy of your methods (for both language models). The accuracy should be computed by the following function, it should be *greater than 0.25*.\n",
    "\n",
    "\n",
    "```python\n",
    "def accuracy(original_sequence, reconstructed_sequence):\n",
    "    sa = original_sequence\n",
    "    sb = reconstructed_sequence\n",
    "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
    "    return score / len(original_sequence)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a158dfd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 2 (6 points)\n",
    "\n",
    "This task is about text generation. You have to:\n",
    "\n",
    "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know.\n",
    "\n",
    "**B**. choose the tokenization procedure. It should have two stages:\n",
    "\n",
    "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure\n",
    "\n",
    "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest prefix of W\n",
    "\n",
    "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
    "\n",
    "1. it should use the RNN language model (trained on sub-word tokens)\n",
    "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
    "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
    "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X) \n",
    "5. in generated texts every token 3-gram should be uniq\n",
    "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from tokenizers import BertWordPieceTokenizer, Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "book_numbers = [2097, 834, 139, 48320, 108, 3289, 2350, 903, 537, 3070, 2347]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "books = [strip_headers(load_etext(book_number)) for book_number in book_numbers]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "840474"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(book.split()) for book in books])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for n, book in enumerate(books):\n",
    "    with open(f'book_{n}.txt', 'w') as f_out:\n",
    "        f_out.write(book)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(books, vocab_size=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.edges = {}\n",
    "        self.is_end = False\n",
    "\n",
    "    def next_tokens(self):\n",
    "        return list(self.edges.keys())\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    def insert(self, tokens):\n",
    "        current_node = self.root\n",
    "        for token in tokens:\n",
    "            if token not in current_node.edges:\n",
    "                current_node.edges[token] = Node()\n",
    "            current_node = current_node.edges[token]\n",
    "        current_node.is_end = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file('tokenizer.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def ids_to_tokens(token_ids):\n",
    "    return [tokenizer.id_to_token(token_id) for token_id in token_ids]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "words = set()\n",
    "for book in books:\n",
    "    encoded = tokenizer.encode(book)\n",
    "    transformed_text = tokenizer.decode(encoded.ids)\n",
    "    words |= set(re.split('\\W+', transformed_text))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "used_ids = set()\n",
    "for word in words:\n",
    "    used_ids |= set(tokenizer.encode(word).ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "all_ids = set(tokenizer.get_vocab().values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "special_ids = []\n",
    "patt = re.compile('#*\\w+')\n",
    "for token_id in (all_ids - used_ids):\n",
    "    token = tokenizer.id_to_token(token_id)\n",
    "    if not patt.match(token):\n",
    "       special_ids.append(token_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['[PAD]',\n '[UNK]',\n '[CLS]',\n '[SEP]',\n '[MASK]',\n '!',\n '\"',\n '$',\n '&',\n \"'\",\n '(',\n ')',\n '*',\n ',',\n '-',\n '.',\n '/',\n ':',\n ';',\n '>',\n '?',\n '[',\n ']',\n '{',\n '}',\n '£',\n '—',\n '‘',\n '’',\n '“',\n '”',\n '″',\n '・',\n '£1']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_tokens(special_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "trie = Trie()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    trie.insert(tokenizer.encode(word).ids)\n",
    "\n",
    "for id in special_ids:\n",
    "    trie.insert([id])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class DoyleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file, tokenizer, sequence_length, device):\n",
    "        txt = open(file).read()\n",
    "\n",
    "        self.encoded = tokenizer.encode(txt)\n",
    "        self.word_indices = self.encoded.ids\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_indices) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.word_indices[index:index+self.sequence_length], device=self.device),\n",
    "            torch.tensor(self.word_indices[index+1:index+self.sequence_length+1], device=self.device)\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "dataset = DoyleDataset('book_0.txt', tokenizer, 15, 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, tokenizer, device):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.embedding_dim = 100\n",
    "        self.num_layers = 2\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        n_vocab = self.tokenizer.get_vocab_size()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
    "\n",
    "    def fit(self, datasets, batch_size=512, max_epochs=30):\n",
    "        self.train()\n",
    "\n",
    "        dataloaders = [DataLoader(dataset, batch_size=batch_size) for dataset in datasets]\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            state_h, state_c = model.init_state(SEQUENCE_LENGTH)\n",
    "\n",
    "            for dataloader in dataloaders:\n",
    "                for batch, (x, y) in enumerate(dataloader):\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "                    loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "                    state_h = state_h.detach()\n",
    "                    state_c = state_c.detach()\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEQUENCE_LENGTH = 15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# model = LSTMModel(tokenizer, device)\n",
    "# model.to(device)\n",
    "#\n",
    "# datasets = [DoyleDataset(f'book_{n}.txt', tokenizer, SEQUENCE_LENGTH, device) for n in range(11)]\n",
    "# model.fit(datasets)\n",
    "# torch.save(model.state_dict(), 'doyle.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(tokenizer, device)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('doyle.model', map_location=torch.device('cpu')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def top_p_sample(logits, temperature, p):\n",
    "    probabilities = softmax(torch.tensor(logits, dtype=torch.int)/temperature, dim=0).detach().cpu().numpy()\n",
    "    indices = np.argsort(probabilities)[::-1]\n",
    "    probabilities = probabilities[indices]\n",
    "    k = np.searchsorted(np.cumsum(probabilities), p) + 1\n",
    "\n",
    "    probabilities = probabilities[:k] / np.sum(probabilities[:k])\n",
    "\n",
    "    index = np.random.choice(indices[:k], p=probabilities)\n",
    "\n",
    "    return index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "def predict(model, prompt, tokens_to_generate, prev_range=5, temperature=0.7, p=0.9):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = model.tokenizer.encode(prompt).ids\n",
    "    state_h, state_c = model.init_state(len(tokens))\n",
    "\n",
    "    current_node = trie.root\n",
    "\n",
    "    unique_3_grams = defaultdict(list)\n",
    "\n",
    "    for i in range(0, tokens_to_generate):\n",
    "        x = torch.tensor([tokens[-prev_range:]])\n",
    "        x = x.to(device)\n",
    "\n",
    "        if current_node.is_end:\n",
    "            allowed_tokens = np.union1d(current_node.next_tokens(), trie.root.next_tokens())\n",
    "        else:\n",
    "            allowed_tokens = np.array(current_node.next_tokens())\n",
    "\n",
    "        allowed_tokens = allowed_tokens.astype(int)\n",
    "\n",
    "        allowed_tokens = np.setdiff1d(allowed_tokens, unique_3_grams[tuple(tokens[-2:])])\n",
    "\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        allowed_logits = last_word_logits.detach().numpy()[allowed_tokens]\n",
    "        index = top_p_sample(allowed_logits, temperature, p)\n",
    "        token_id = allowed_tokens[index]\n",
    "\n",
    "        if token_id in current_node.next_tokens():\n",
    "            current_node = current_node.edges[token_id]\n",
    "        else:\n",
    "            current_node = trie.root.edges[token_id]\n",
    "\n",
    "        unique_3_grams[tuple(tokens[-2:])].append(token_id)\n",
    "        tokens.append(token_id)\n",
    "\n",
    "    return model.tokenizer.decode(tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "'on reaching london i drove a first time before, so as to see it. the whole business, however, was more genuine than ever, and the incident gave a case. \" yes, i am a hard, \" said he. \" you have had uneasy, watson, we shall have a word. \" \" my dear fellow, you are not yourself. \" holmes leaned back in his chair. \" oh, my dear watson, i have no doubt that you will know that i have said this very hour. \" i had a long way of his heart. \" here\\'s a friend! \" said i. \" my god! \" i cried. \" what do you think of the attempted? ” “ i can give you a sketch of the name. \" the baronet\\'s voice was not unlike a hound. \" we\\'ve been in the bed. \" he spoke in a feeble voice, but he was at the back of his voice. \" now, watson. i will watch you. \" “ i must confess that i would have you for your future acquiescence. \" chapter 14 the hound of the baskervilles holmes. \" then he lied to me, holmes. i could not forget the manager of his head. but the instant i tried to impress him out in a cold face, uncertain as i did so. i was bitterly hurt - hearted and breathing, but it was evident the condition of a man, as he was satisfied. \" well, watson! \" cried holmes. “ my dear chap, he is a son of hers. but how could you have told you that you have done? \" \" that is certainly an extraordinary thing. you have no means of foreseeing from the andamans of london. \" his only action was a man of considerable agitation. \" really, watson ; you must help me, for he will pretend to help me. i\\'ll forget it. \" it was rankled by his death. i had forgotten my wife and let the day rest. \" so i did not know that the opportunity were of his own. \" she crept away from the bedside, and he muttered his face. \" give me the good letters, lestrade, \" he cried. “ your rooms are full of interest. \" but the baronet was standing in the low, cracked voice which i had poured up by the fire. \" there\\'s no doubt you\\'ll have a cab with you, \" i answered. \" and then you have been on your side of me?'"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"On reaching London I drove\", 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "'i turned my lantern down the the the of four. mortimer\\'s, of the stapletons, of merripit house. a cold sweat bathed from the influence of paper upon the table. holmes sprang out of bed, but on the passage he sprang out upon his hands. \" you can understand, watson, that you have not aided me, \" she said. \" and leave you to see that the disguise of yours is suggested by the\\'80. \" \" i\\'ll help you, watson. you will find your own nerves\\'s work. \" holmes leaned forward and laid it open. \" thank you. \" he looked down at my companion. \" what about this, holmes? ” he asked. the old gentleman nodded. \" why should i demonstrate you? \" \" yes, i have had a few weeks. \" she glanced at me in amazement. \" well, then, you can do, \" holmes answered. \" i give you my word that you will have a good deal to worry. \" the same thought became intent. \" a nice villain! \" she cried. \" it is a very curious world. \" his eyes shone upon me. \" my word, holmes, \" he whispered. \" that is your letter. i\\'ve had a wire, watson! \" he spoke in a rigid way. \" no doubt you will keep your mind. \" at least i have seen such a feat with me. my friend\\'s death was weak and always. \" now, watson - - - and - - anywhere? \" he asked, gasping for breath. \" perhaps you would care to help me. i will not mention my case. let us see how it is. \" so i heard him say. \" after all i have ever to do, but i give it my word. \" stapleton laughed. \" if you think you\\'- - \" \" you have only had a good night at your watch. \" your suggestion was weak in the very centre of the room. \" yes. \" there was a hundred tin box which i had composed all. in the midst of the second i had sprung to town. my nerves tingled with the somewhat shock, and i clapped myself by my bed. i could not help remarking staring at it, and so determined i had left him, and the man was waiting for me. it was not for the truth, however, that i had heard of my unhappy companion. “ i could imagine my mistress, \" said i. \"'"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"i turned my lantern down\", 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "'i turned my lantern down as he cried i heard the striking silver of my revolver. \" you will excuse me, watson. you have no respect for me. \" \" i can hardly imagine, \" i explained. \" well, i shouldn\\'t be surprised, madam. we can only have my case in any case. but you are a visitor, dr. watson, but you will see the situation. \" holmes groaned. \" get your distance by any means. i don\\'t know! \" \" a good deal. \" the fog of the fanlight had been a case. \" my word! \" he cried. \" but why? \" \" ah, you will know that it is difficult. you can drive a savage, fixed yourself, and you have to fear. \" i could see the gleam of keys and of furniture. \" a hound, watson, \" he said. \" is he delirious? \" holmes looked up and glanced keenly into my chair. \" now, watson! \" she cried. i could not help him. \" oh, it is painful to have been, but it is not my duty. it is time that i have spoken to you. i will promise you that it will not be the condition. if you will persuade me to get down to - morrow morning, watson — you can hardly expect me to solve my duty to help you. \" he relaxed a glass of puzzled, but the room was covered with a shrill scream. \" why should i be perfectly frank? \" she asked defiantly. \" and the high scientific particular man who seems to be the heir of england, and he has no recollection of the death of his death. \" chapter 14 the hound of the baskervilles he was lifted to the station. \" yes, yes, i have a case for some time. \" she was deeply exhausted. i was still rather raw for a surprising time. it was the brightness of fever, and the sick man who had brought him so long. in a few minutes he was silent. he was a long, reckless - looking fellow, who was smartly dressed, with his head sunk upon the floor, and then, rushing forward, he gave an exclamation of surprise. \" that\\'s why i have been talking to you, \" said he. \" it will be as well that i should lose hurt in case all that you have. \" stapleton looked at me with venomous eyes. \" do you think that you are an honest woman? \" i asked. \" no,'"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"i turned my lantern down\", 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "41e84582",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 3\n",
    "\n",
    "In this task you have to create a network which looks at characters of the word and tries to guess whether the word is a noun, a verb, an adjective, and so on. To be more precise: the input is a word (without context), the output is a POS-tag (Part-of-Speech). Since some words are unambiguous, and we have no context, our network is supposed to return the set of possible tags.\n",
    "\n",
    "The data is taken from Universal Dependencies English corpus, and of course it contains errors, especially because not all possible tags occured in the data.\n",
    "\n",
    "Train a network (4p) or two networks (+2p) solving this task. Both networks should look at character n-grams occuring in the word. There are two options:\n",
    "\n",
    "* **Fixed size:** for instance take 2,3, and 4-character suffixes of the word, use them as  features (whith 1-hot encoding). You can also combine prefix and suffix features. Simple, useful trick: when looking at suffixes, add some '_' characters at the beginning of the word to guarantee that shorter words have suffixes of a desired length.\n",
    "\n",
    "* **Variable size:** take for instance 4-grams (or 4 grams and 3-grams), use Deep Averaging Network. Simple trick: add extra character at the beginning and at the end of the word, to add the information, that ngram occurs at special position ('ed' at the end has slightly different meaning that 'ed' in the middle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac85fcb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 4\n",
    "\n",
    "Apply seq2seq model (you can modify the code from this tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) to compute grapheme to phoneme conversion for English. Train the model on dev_cmu_dict.txt and test it on test_cmu_dict.txt. Report accuracy of your solution using two metrics:\n",
    "* exact match (how many words are perfectly converted to phonemes)\n",
    "* exact match without stress (how many words are perfectly converted to phonemes when we remove the information about stress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da193ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d5e47",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538fb76",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4feefe2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}