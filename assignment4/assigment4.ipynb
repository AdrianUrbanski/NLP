{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef34449",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assigment 4\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* get at least 4 points by Tuesday, 12.05.2022\n",
    "* remaining points: last lab session before or on Tuesday, 19.05.2022\n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1HaMbhzaBxxNa_z_QJXSDCbv5VddmhVVZ?usp=sharing> (or will be soon :) )\n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss from Slide 3 in Lecture NLP.2, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_pdf = pd.read_csv('task1_objects_contexts_polish.txt', sep=' ', names=['object', 'context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "object_encoder = LabelEncoder()\n",
    "data_pdf['object'] = object_encoder.fit_transform(data_pdf['object'])\n",
    "\n",
    "context_encoder = LabelEncoder()\n",
    "data_pdf['context'] = context_encoder.fit_transform(data_pdf['context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, data_pdf):\n",
    "        self.positive_samples = data_pdf.groupby('context').apply(lambda pdf: set(pdf['object'].unique()))\n",
    "        self.object_distribution = np.power(data_pdf['object'].value_counts(), 3/4)\n",
    "        self.all_samples = set(data_pdf['object'].unique())\n",
    "\n",
    "    def sample(self, context, k):\n",
    "        negative_samples = list(self.all_samples - self.positive_samples[context])\n",
    "        negative_distribution = self.object_distribution[negative_samples]\n",
    "        negative_distribution /= np.sum(negative_distribution)\n",
    "        return np.random.choice(negative_distribution.index, size=k, replace=False, p=negative_distribution.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def object_grad(object_embedding, context_embedding):\n",
    "    return -(1-sigmoid(np.dot(object_embedding, context_embedding)))*context_embedding\n",
    "\n",
    "def context_grad(object_embedding, context_embedding, negative_embeddings):\n",
    "    return (-(1-sigmoid(object_embedding@context_embedding))*object_embedding\n",
    "            + np.sum((1-sigmoid(-context_embedding.reshape(1, -1) @ negative_embeddings.T))*negative_embeddings.T, axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def loss_fun(object_embedding, context_embedding, negative_embeddings):\n",
    "    return (-np.log(sigmoid(object_embedding @ context_embedding))\n",
    "            - np.sum(sigmoid(-context_embedding.reshape(1, -1) @ negative_embeddings.T)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, lr: float, k: int, embeddings_size: int):\n",
    "        self.lr = lr\n",
    "        self.k = k\n",
    "        self.object_embeddings = np.random.rand(len(data_pdf['object'].unique()), embeddings_size)\n",
    "        self.context_embeddings = np.random.rand(len(data_pdf['context'].unique()), embeddings_size)\n",
    "\n",
    "    def step(self, object, context, samples):\n",
    "        negative_objects = np.array(samples)\n",
    "        self.object_embeddings[object] -= self.lr*object_grad(self.object_embeddings[object], self.context_embeddings[context])\n",
    "        self.context_embeddings[context] -= self.lr*context_grad(self.object_embeddings[object], self.context_embeddings[context], self.object_embeddings[negative_objects])\n",
    "        return loss_fun(self.object_embeddings[object], self.context_embeddings[context], self.object_embeddings[negative_objects])\n",
    "\n",
    "    def run_epoch(self, data, log_freq, distribution):\n",
    "        negative_samples = np.random.choice(distribution.index, p=distribution.values, size=(len(data), self.k))\n",
    "        iter = 0\n",
    "        loss = 0\n",
    "        for object, context in tqdm(data):\n",
    "            loss += self.step(object, context, negative_samples[iter])\n",
    "            iter += 1\n",
    "            if log_freq is not None and iter % log_freq == 0:\n",
    "                print('loss: ', loss/iter)\n",
    "\n",
    "    def fit(self, data_pdf, num_epochs, log_freq = 1000000):\n",
    "        distribution = np.power(data_pdf['object'].value_counts(), 3/4)\n",
    "        distribution /= sum(distribution)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(epoch)\n",
    "            self.run_epoch(data_pdf.values, log_freq, distribution)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d3a02406e784a36bc662327f0f49093"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [27]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m word2vec \u001B[38;5;241m=\u001B[39m Word2Vec(\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mword2vec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_pdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mWord2Vec.fit\u001B[0;34m(self, data_pdf, num_epochs, log_freq)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28mprint\u001B[39m(epoch)\n\u001B[0;32m---> 29\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_pdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistribution\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mWord2Vec.run_epoch\u001B[0;34m(self, data, log_freq, distribution)\u001B[0m\n\u001B[1;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mobject\u001B[39m, context \u001B[38;5;129;01min\u001B[39;00m tqdm(data):\n\u001B[0;32m---> 19\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnegative_samples\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m%\u001B[39m log_freq \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mWord2Vec.step\u001B[0;34m(self, object, context, samples)\u001B[0m\n\u001B[1;32m      9\u001B[0m negative_objects \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(samples)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobject_embeddings[\u001B[38;5;28mobject\u001B[39m] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr\u001B[38;5;241m*\u001B[39mobject_grad(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobject_embeddings[\u001B[38;5;28mobject\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_embeddings[context])\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_embeddings[context] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr\u001B[38;5;241m*\u001B[39m\u001B[43mcontext_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobject_embeddings\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext_embeddings\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobject_embeddings\u001B[49m\u001B[43m[\u001B[49m\u001B[43mnegative_objects\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss_fun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobject_embeddings[\u001B[38;5;28mobject\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_embeddings[context], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobject_embeddings[negative_objects])\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mcontext_grad\u001B[0;34m(object_embedding, context_embedding, negative_embeddings)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontext_grad\u001B[39m(object_embedding, context_embedding, negative_embeddings):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;241m-\u001B[39m(\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39msigmoid(object_embedding\u001B[38;5;129m@context_embedding\u001B[39m))\u001B[38;5;241m*\u001B[39mobject_embedding\n\u001B[0;32m----> 6\u001B[0m             \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39msum((\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39msigmoid(\u001B[38;5;241m-\u001B[39m\u001B[43mcontext_embedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;241m@\u001B[39m negative_embeddings\u001B[38;5;241m.\u001B[39mT))\u001B[38;5;241m*\u001B[39mnegative_embeddings\u001B[38;5;241m.\u001B[39mT, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(0.01, 5, 100)\n",
    "word2vec.fit(data_pdf, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "embeddings = word2vec.object_embeddings\n",
    "embeddings_pdf = pd.DataFrame(embeddings)\n",
    "embeddings_pdf.index = object_encoder.inverse_transform(embeddings_pdf.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "with open('task1_w2v_vectors.txt', 'w') as out_file:\n",
    "    out_file.write(f'{len(embeddings_pdf)} {len(embeddings_pdf.columns)}\\n')\n",
    "    embeddings_pdf.to_csv(out_file, header=False, index=True, sep=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ae909d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    piaśnik 0.6282005906105042\n",
      "    humoreska 0.6244557499885559\n",
      "    marnotrawca 0.6106693148612976\n",
      "    piśmienność 0.610500156879425\n",
      "    konkieta 0.6069074869155884\n",
      "    przodownica 0.6068059802055359\n",
      "    herling 0.605897068977356\n",
      "    pleciuga 0.6058119535446167\n",
      "    wyścigówka 0.6022748947143555\n",
      "    pokemon 0.5997951626777649\n",
      "\n",
      "WORD: smok\n",
      "    smużka 0.7996348738670349\n",
      "    rozniesienie 0.7966349720954895\n",
      "    nieporównywalność 0.7896068096160889\n",
      "    brzegówka 0.7860226035118103\n",
      "    brzeźnica 0.7849063277244568\n",
      "    kardiochirurgia 0.7835286855697632\n",
      "    kleń 0.7822569608688354\n",
      "    zrywność 0.7816804647445679\n",
      "    kanelura 0.7801304459571838\n",
      "    sandor 0.7800078988075256\n",
      "\n",
      "WORD: miłość\n",
      "    przeszczep 0.4740453362464905\n",
      "    włościaństwo 0.46320226788520813\n",
      "    byt 0.45898422598838806\n",
      "    kabul 0.4547436833381653\n",
      "    zrównoważanie 0.4523521363735199\n",
      "    belka 0.4435618221759796\n",
      "    wyminięcie 0.44016098976135254\n",
      "    owca 0.440153568983078\n",
      "    kulka 0.4397164583206177\n",
      "    poderwanie 0.439565509557724\n",
      "\n",
      "WORD: rower\n",
      "    pokaszliwanie 0.7669849991798401\n",
      "    kamaryla 0.75851970911026\n",
      "    szeremietiew 0.7567898631095886\n",
      "    wyprzedanie 0.752396285533905\n",
      "    przychwycenie 0.7496243715286255\n",
      "    ekshibicjonista 0.7471175193786621\n",
      "    dengler 0.7470254302024841\n",
      "    poczciwina 0.7465839385986328\n",
      "    trznadel 0.7461940050125122\n",
      "    zapęd 0.7437562346458435\n",
      "\n",
      "WORD: maraton\n",
      "    sulikowski 0.8112579584121704\n",
      "    cybeta 0.8082464933395386\n",
      "    tamiza 0.7973549365997314\n",
      "    baletka 0.7966172099113464\n",
      "    lupa 0.7936609387397766\n",
      "    plecenie 0.7919416427612305\n",
      "    koteczek 0.7918727993965149\n",
      "    kwiatuszek 0.7908705472946167\n",
      "    wyszczególnianie 0.7904800176620483\n",
      "    hołysz 0.7900405526161194\n",
      "\n",
      "WORD: logika\n",
      "    spermatogeneza 0.7073168158531189\n",
      "    jenek 0.7022218108177185\n",
      "    doszukiwanie 0.6951398849487305\n",
      "    rufin 0.6949042081832886\n",
      "    rodziciel 0.6943486928939819\n",
      "    szantymen 0.6934229731559753\n",
      "    demodulacja 0.6925517320632935\n",
      "    powarkiwanie 0.6924922466278076\n",
      "    neurotyk 0.6893442273139954\n",
      "    porcelit 0.6886489391326904\n",
      "\n",
      "WORD: motyl\n",
      "    termowentylator 0.8246421813964844\n",
      "    żychowski 0.8244712948799133\n",
      "    wór 0.8227187395095825\n",
      "    skarmianie 0.8177772164344788\n",
      "    zachar 0.815920889377594\n",
      "    przyglądnięcie 0.8150855302810669\n",
      "    fronda 0.8150303959846497\n",
      "    wywleczenie 0.8147520422935486\n",
      "    pięciogroszówka 0.8116776347160339\n",
      "    brewer 0.8112464547157288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors.txt', binary=False)\n",
    "\n",
    "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
    "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "example_words = example_polish_words\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task1_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2 (4 points)\n",
    "\n",
    "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
    "\n",
    "```python\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "```\n",
    "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file] containing such pairs of titles, that one article links to another.\n",
    "\n",
    "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
    "\n",
    "Compare these two approaches using similar code to the code from Task 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6ad8e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wiki_links_pdf = pd.read_csv('task2_simple.wiki.links.txt', sep=' ', names=['object', 'context'])\n",
    "# The cell for your presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "wiki_titles = set(np.hstack((wiki_links_pdf['object'], wiki_links_pdf['context'])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "word_counts = pd.Series(list(wiki_titles)).apply(lambda title: str(title).split('_')).explode().value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "rare_words = word_counts[(word_counts > 1) & (word_counts <= 30)].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79ac7e430df242c588ae4f18a1a2c4f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles = defaultdict(list)\n",
    "wiki_titles = list(wiki_titles)\n",
    "rare_words = set(rare_words)\n",
    "\n",
    "for it, title in tqdm(enumerate(wiki_titles)):\n",
    "    for word in list(set(str(title).split('_')) & rare_words):\n",
    "        articles[word].append(it)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/204926 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb0c898d45b840e49fa50c51d5c27b29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task2_related_wiki_links.txt', 'w') as out_file:\n",
    "    for vals in tqdm(articles.values()):\n",
    "        for pair in combinations(vals, 2):\n",
    "            out_file.write(f'{wiki_titles[pair[0]]} {wiki_titles[pair[1]]}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "example_titles = [\n",
    "    'capital_city',\n",
    "    'flower',\n",
    "    'mickey_mouse',\n",
    "    'finance',\n",
    "    'japan_national_under-23_football_team',\n",
    "    'john_the_apostle',\n",
    "    'boss_(gaming)',\n",
    "    'the_wealth_of_nations',\n",
    "    'sulfur_dioxide',\n",
    "    'tandem_bicycle',\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_simple.wiki.links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: capital_city\n",
      "    population_density 0.739251434803009\n",
      "    province 0.7069327235221863\n",
      "    plain 0.701899528503418\n",
      "    oceanic_climate 0.6916329264640808\n",
      "    köppen_climate_classification 0.6897584199905396\n",
      "    list_of_capital_cities_by_altitude 0.6814950704574585\n",
      "    category:departments_of_paraguay 0.6782993674278259\n",
      "    above_sea_level 0.6735855340957642\n",
      "    category:departments_of_the_republic_of_the_congo 0.6711230874061584\n",
      "    hill 0.669350266456604\n",
      "\n",
      "WORD: flower\n",
      "    asteraceae 0.8791819214820862\n",
      "    leaf 0.8778598308563232\n",
      "    berry 0.8747152090072632\n",
      "    vine 0.8712254166603088\n",
      "    seed 0.8700944781303406\n",
      "    perennial 0.8616876602172852\n",
      "    tail 0.8591261506080627\n",
      "    fruit 0.856979489326477\n",
      "    sweet_(taste) 0.8559210300445557\n",
      "    lime 0.8551871180534363\n",
      "\n",
      "WORD: mickey_mouse\n",
      "    minnie_mouse 0.9010185599327087\n",
      "    goofy 0.8845751285552979\n",
      "    bugs_bunny 0.8688761591911316\n",
      "    donald_duck 0.8686318397521973\n",
      "    daffy_duck 0.857617974281311\n",
      "    pluto_(disney) 0.8547376990318298\n",
      "    walt_disney_company 0.8538717031478882\n",
      "    tokyo_disneyland 0.8529824614524841\n",
      "    daisy_duck 0.8525812029838562\n",
      "    fred_moore_(animator) 0.8519846796989441\n",
      "\n",
      "WORD: finance\n",
      "    investment 0.859107494354248\n",
      "    banking 0.8380959630012512\n",
      "    insurance 0.828545868396759\n",
      "    stock_market 0.8262161016464233\n",
      "    financial 0.819246232509613\n",
      "    auction 0.8171119689941406\n",
      "    shareholder 0.8169492483139038\n",
      "    business 0.8091278076171875\n",
      "    share_(finance) 0.8073405623435974\n",
      "    wikt:found 0.8066060543060303\n",
      "\n",
      "WORD: japan_national_under-23_football_team\n",
      "    hong_kong_national_football_team 0.9604832530021667\n",
      "    2011_afc_asian_cup 0.9599856734275818\n",
      "    football_at_the_1966_asian_games 0.9528710246086121\n",
      "    2019_afc_asian_cup 0.951862633228302\n",
      "    football_at_the_1970_asian_games 0.9510948061943054\n",
      "    1988_afc_asian_cup 0.9494122862815857\n",
      "    football_at_the_1986_asian_games 0.9492978453636169\n",
      "    2000_afc_asian_cup 0.948467493057251\n",
      "    football_at_the_2002_asian_games_–_men's_tournament 0.9473084211349487\n",
      "    football_at_the_1978_asian_games 0.9464484453201294\n",
      "\n",
      "WORD: john_the_apostle\n",
      "    gospel_of_mark 0.978923499584198\n",
      "    acts_of_the_apostles 0.9740555286407471\n",
      "    matthew_the_evangelist 0.9712485671043396\n",
      "    jerome 0.9652677774429321\n",
      "    category:ancient_christianity 0.9603877663612366\n",
      "    theotokos 0.960051417350769\n",
      "    nicene_creed 0.9586400389671326\n",
      "    gospel_of_luke 0.9579897522926331\n",
      "    origen 0.9572450518608093\n",
      "    paul_of_tarsus 0.9551813006401062\n",
      "\n",
      "WORD: boss_(gaming)\n",
      "    nascar_07 0.9553720951080322\n",
      "    list_of_acclaim_entertainment_subsidiaries#acclaim_studios_cheltenham 0.9435751438140869\n",
      "    onlive 0.9433116912841797\n",
      "    british_academy_games_awards 0.942517876625061\n",
      "    megatech 0.9415945410728455\n",
      "    conficker_a 0.9402057528495789\n",
      "    barney_calhoun 0.9397355914115906\n",
      "    cs_go 0.9394882321357727\n",
      "    barbie_horse_adventures_ii:_riding_camp 0.9381359815597534\n",
      "    14_degrees_east 0.937997579574585\n",
      "\n",
      "WORD: the_wealth_of_nations\n",
      "    ricardian_model 0.952191174030304\n",
      "    neoclassical_economics 0.9482550621032715\n",
      "    the_theory_of_moral_sentiments 0.9475444555282593\n",
      "    classical_economics 0.9471796751022339\n",
      "    category:anarchism 0.943265438079834\n",
      "    economies_of_scale 0.9426497220993042\n",
      "    public_good 0.9414759874343872\n",
      "    category:economic_theories 0.9397423267364502\n",
      "    category:economic_systems 0.9373308420181274\n",
      "    open_market 0.9350383281707764\n",
      "\n",
      "WORD: sulfur_dioxide\n",
      "    crystallization 0.9755793213844299\n",
      "    sulfuric_acid 0.9725997447967529\n",
      "    magnetite 0.9715259075164795\n",
      "    specific_gravity 0.9689416289329529\n",
      "    malleable 0.967831552028656\n",
      "    sodium_chloride 0.9677532911300659\n",
      "    titanium_dioxide 0.967487633228302\n",
      "    uranyl_nitrate 0.9673746824264526\n",
      "    carbonic_acid 0.966763436794281\n",
      "    carbon_steel 0.9667571187019348\n",
      "\n",
      "WORD: tandem_bicycle\n",
      "    elena_marsili 0.9514120221138\n",
      "    crown_prince_of_greece 0.95046466588974\n",
      "    category:tunisian_producers 0.9477298259735107\n",
      "    jerzy_kowalczyk 0.9473161101341248\n",
      "    category:belgian_atheists 0.9470301866531372\n",
      "    josé_agustín_goytisolo 0.9466261267662048\n",
      "    category:guyanese_writers 0.9465793371200562\n",
      "    swimming_at_the_1992_summer_paralympics 0.9458359479904175\n",
      "    premio_alfaguara 0.9450024962425232\n",
      "    mingo_y_aníbal_contra_los_fantasmas 0.9446794986724854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_simple_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task2_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_related_wiki_links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: capital_city\n",
      "capital_city not found in the training set\n",
      "\n",
      "WORD: flower\n",
      "flower not found in the training set\n",
      "\n",
      "WORD: mickey_mouse\n",
      "mickey_mouse not found in the training set\n",
      "\n",
      "WORD: finance\n",
      "finance not found in the training set\n",
      "\n",
      "WORD: japan_national_under-23_football_team\n",
      "japan_national_under-23_football_team not found in the training set\n",
      "\n",
      "WORD: john_the_apostle\n",
      "john_the_apostle not found in the training set\n",
      "\n",
      "WORD: boss_(gaming)\n",
      "    first-person_(gaming) 0.974440336227417\n",
      "    item_(gaming) 0.97352534532547\n",
      "    match_(gaming) 0.9721232056617737\n",
      "    ken_williams_(gaming) 0.971487283706665\n",
      "    deathmatch_(gaming) 0.9703531861305237\n",
      "    spam_(gaming) 0.966529130935669\n",
      "    kana_kitahara 0.9655419588088989\n",
      "    stalling_(gaming) 0.9654951691627502\n",
      "    first_person_(gaming) 0.9649636149406433\n",
      "    toa_alta,_puerto_rico 0.9643204212188721\n",
      "\n",
      "WORD: the_wealth_of_nations\n",
      "    wealth_management 0.9905750751495361\n",
      "    morgan_stanley_wealth_management 0.9884107112884521\n",
      "    list_of_countries_by_distribution_of_wealth 0.9877914190292358\n",
      "    appeal_to_wealth 0.9875590801239014\n",
      "    wealth_(movie) 0.9873790144920349\n",
      "    gross_domestic_product#standard_of_living_and_gdp:_wealth_distribution_and_externalities 0.9867001175880432\n",
      "    sovereign_wealth_fund 0.9865601658821106\n",
      "    evercore_wealth_management 0.9864317178726196\n",
      "    wealth_gap_in_the_united_states 0.986093282699585\n",
      "    wealth_redistribution 0.9860005378723145\n",
      "\n",
      "WORD: sulfur_dioxide\n",
      "    image:mauna_loa_sulfur_hexafluoride_concentration.jpg 0.9789128303527832\n",
      "    sulfur_trioxide 0.9754117131233215\n",
      "    rhenium_dioxide 0.974718451499939\n",
      "    bromine_dioxide 0.9737715125083923\n",
      "    sulfur_pentafluoride 0.973708987236023\n",
      "    sulfur_monoxide 0.9730807542800903\n",
      "    selenium_dioxide 0.9724143743515015\n",
      "    sulfur_dichloride 0.9720863103866577\n",
      "    sulfur_nitride 0.9719774127006531\n",
      "    sulfur_tetrachloride 0.97173672914505\n",
      "\n",
      "WORD: tandem_bicycle\n",
      "    amino_acids 0.9693626165390015\n",
      "    argonne_tandem_linear_accelerator_system 0.9685113430023193\n",
      "    tandem_repeat 0.9683820605278015\n",
      "    aren't_you_glad_you're_you? 0.9681868553161621\n",
      "    akari_inaba 0.9663731455802917\n",
      "    retro_vhs_blu-ray 0.9661436676979065\n",
      "    recommended_citation:_frost,_darrel_r._2019._amphibian_species_of_the_world:_an_online_reference._version_6.0_(date_of_access)._electronic_database_accessible_at_http://research.amnh.org/herpetology/amphibia/index.html._american_museum_of_natural_history,_new_york,_usa. 0.9659613966941833\n",
      "    file:us_navy_070412-n-7498l-018_a_french_super-etendard_from_the_nuclear-powered_aircraft_carrier_french_navy_ship_charles_de_gaulle_(r_91)_performs_a_touch-and-go_landing_on_the_flight_deck_of_the_nimitz-class_aircraft_carrier_uss.jpg 0.965947687625885\n",
      "    bitlis_vilayet 0.965805172920227\n",
      "    drying_rack 0.9657403826713562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_related_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    try:\n",
    "        for w, v in task2_wv.most_similar(w):\n",
    "            print ('   ', w, v)\n",
    "    except:\n",
    "        print(f'{w} not found in the training set')\n",
    "    print ()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import shutil\n",
    "with open('task2_combined_wiki_links.txt', 'wb') as out_file:\n",
    "    for file in ['task2_related_wiki_links.txt', 'task2_simple.wiki.links.txt' ]:\n",
    "        with open(file, 'rb') as in_file:\n",
    "            shutil.copyfileobj(in_file, out_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_combined_wiki_links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [41]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m task2_wv \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mwv\n\u001B[1;32m      2\u001B[0m task2_wv\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtask2_combined_wiki_links_w2v_vectors.txt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m example_words \u001B[38;5;241m=\u001B[39m \u001B[43mexample_titles\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m example_words:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWORD:\u001B[39m\u001B[38;5;124m'\u001B[39m, w)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'example_titles' is not defined"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_combined_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print('WORD:', w)\n",
    "    try:\n",
    "        for w, v in task2_wv.most_similar(w):\n",
    "            print('   ', w, v)\n",
    "    except:\n",
    "        print(f'{w} not found in the training set')\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "d711fdca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 3 (4 points)\n",
    "\n",
    "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
    "\n",
    "<pre>\n",
    "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "</pre>\n",
    "\n",
    "And this is its translation into Lower:\n",
    "\n",
    "<pre>\n",
    "the quick brown fox jumps over the lazy dog\n",
    "</pre>\n",
    "\n",
    "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
    "\n",
    "Prepare the corpora wich contains three kind of sentences:\n",
    "* Upper corpus sentences\n",
    "* Lower corpus sentences\n",
    "* sentences derived from Upper/Lower corpus, modified using D\n",
    "\n",
    "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
    "\n",
    "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
    "\n",
    "<pre>\n",
    "WOLF, CAT, WOLVES, LION, gopher, dog\n",
    "</pre>\n",
    "\n",
    "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('task3_polish_lower.txt', 'r') as in_file:\n",
    "    lower_sentences = in_file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "lower_sentences = lower_sentences.split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "lower_word_counts = pd.Series(lower_sentences).apply(lambda sentence: str(sentence).split(' ')).explode().value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task3_polish_lower.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    kot_kota 0.6890381574630737\n",
      "    kot 0.68598473072052\n",
      "    koń 0.6662598848342896\n",
      "    zwierzyć_zwierzę 0.6409239172935486\n",
      "    dziewczyna 0.6219024658203125\n",
      "    ubranie 0.6122429966926575\n",
      "    dzieciak 0.6119948625564575\n",
      "    chłopak 0.6075925230979919\n",
      "    chłopiec 0.6048930883407593\n",
      "    pokarm_pokarmić 0.6029188632965088\n",
      "\n",
      "WORD: smok\n",
      "    słoń 0.6976077556610107\n",
      "    demon 0.6848234534263611\n",
      "    byk 0.6831806302070618\n",
      "    bogini 0.6762536764144897\n",
      "    kontur 0.6749581098556519\n",
      "    szatan 0.6743872165679932\n",
      "    płomień 0.6717569231987\n",
      "    potwór 0.6615457534790039\n",
      "    anioł 0.661408007144928\n",
      "    posąg 0.6563248038291931\n",
      "\n",
      "WORD: miłość\n",
      "    bóg 0.753804087638855\n",
      "    dobroć 0.7141699194908142\n",
      "    dusza 0.706268310546875\n",
      "    przyjaźnić_przyjaźń 0.6991156935691833\n",
      "    wiara 0.6935740113258362\n",
      "    zbawienie 0.6916703581809998\n",
      "    mądrość 0.6714519262313843\n",
      "    uczucie 0.6710386872291565\n",
      "    zmartwychwstanie_zmartwychwstać 0.6666459441184998\n",
      "    życzliwość 0.6620675325393677\n",
      "\n",
      "WORD: rower\n",
      "    motocykl 0.7685192823410034\n",
      "    pieszy 0.6830376982688904\n",
      "    rolka 0.6787105798721313\n",
      "    samochód 0.6707159876823425\n",
      "    autobus 0.6694546341896057\n",
      "    wózek 0.661787211894989\n",
      "    narta 0.6470047235488892\n",
      "    wóz 0.6336982250213623\n",
      "    chodnik 0.6329148411750793\n",
      "    przejażdżka 0.6327358484268188\n",
      "\n",
      "WORD: maraton\n",
      "    gal_gala 0.767632782459259\n",
      "    rajd 0.734879732131958\n",
      "    festiwal 0.6953163146972656\n",
      "    wyścig 0.6875052452087402\n",
      "    plebiscyt 0.6830359697341919\n",
      "    dożynki 0.6751199960708618\n",
      "    koncert 0.6713299751281738\n",
      "    zmaganie 0.6713123321533203\n",
      "    olimpiada 0.6668050289154053\n",
      "    smackdown 0.6639860272407532\n",
      "\n",
      "WORD: logika\n",
      "    idea 0.5686460137367249\n",
      "    koncepcja 0.53615403175354\n",
      "    wewnętrznie 0.5270957946777344\n",
      "    wizja 0.5074282884597778\n",
      "    ideał 0.5070559978485107\n",
      "    zamysł 0.5042352080345154\n",
      "    filozofia 0.5030335783958435\n",
      "    sens 0.5022227764129639\n",
      "    intencja 0.5013389587402344\n",
      "    ustroić_ustrój 0.49138882756233215\n",
      "\n",
      "WORD: motyl\n",
      "    ziarnisty 0.7586783170700073\n",
      "    1646 0.7376838326454163\n",
      "    palić_palma 0.7330502271652222\n",
      "    pomarszczony 0.7302441000938416\n",
      "    ricardo 0.7290982604026794\n",
      "    friends 0.7262406945228577\n",
      "    evo 0.7181618809700012\n",
      "    uzbecki 0.7172012329101562\n",
      "    durham 0.716585636138916\n",
      "    lower 0.7161213755607605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task3_wv = model.wv\n",
    "\n",
    "example_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task3_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "lower_kmeans = KMeans(n_clusters=100)\n",
    "preds = lower_kmeans.fit_predict(task3_wv.vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "preds_pdf = pd.DataFrame(preds, columns=['cluster'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "preds_pdf.index = task3_wv.index_to_key"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "preds_pdf = preds_pdf.join(lower_word_counts.rename('count'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "cluster_representatives_pdf = preds_pdf.groupby('cluster').apply(lambda pdf: pdf.sort_values('count', ascending=False).iloc[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                        cluster  count\ncluster                               \n0       śmierć                0   1360\n        św.                   0   1119\n        ojciec                0   1038\n        matka                 0   1012\n        krzyż                 0    969\n...                         ...    ...\n99      niemcy_niemiec       99   1348\n        zjednoczony          99   1217\n        iii                  99   1159\n        zachodni             99   1150\n        południowy           99   1150\n\n[975 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>cluster</th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>cluster</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>śmierć</th>\n      <td>0</td>\n      <td>1360</td>\n    </tr>\n    <tr>\n      <th>św.</th>\n      <td>0</td>\n      <td>1119</td>\n    </tr>\n    <tr>\n      <th>ojciec</th>\n      <td>0</td>\n      <td>1038</td>\n    </tr>\n    <tr>\n      <th>matka</th>\n      <td>0</td>\n      <td>1012</td>\n    </tr>\n    <tr>\n      <th>krzyż</th>\n      <td>0</td>\n      <td>969</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">99</th>\n      <th>niemcy_niemiec</th>\n      <td>99</td>\n      <td>1348</td>\n    </tr>\n    <tr>\n      <th>zjednoczony</th>\n      <td>99</td>\n      <td>1217</td>\n    </tr>\n    <tr>\n      <th>iii</th>\n      <td>99</td>\n      <td>1159</td>\n    </tr>\n    <tr>\n      <th>zachodni</th>\n      <td>99</td>\n      <td>1150</td>\n    </tr>\n    <tr>\n      <th>południowy</th>\n      <td>99</td>\n      <td>1150</td>\n    </tr>\n  </tbody>\n</table>\n<p>975 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_representatives_pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "representatives = [idx[1] for idx in cluster_representatives_pdf.index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "representatives = list(filter(lambda word: word.isalnum() and not word.isnumeric(), representatives))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "repr_sentences = defaultdict(list)\n",
    "representatives = set(representatives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "with open('task3_representatives.txt', 'w') as out_file:\n",
    "    out_file.write(' '.join(representatives))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "for sentence in lower_sentences:\n",
    "    sentence_words = set(sentence.split(' '))\n",
    "    for word in list(sentence_words & representatives):\n",
    "        repr_sentences[word].append(sentence)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/836 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b97f13191664d5cb3cf89427ba151a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_lower_to_upper.txt', 'w') as out_file:\n",
    "    for representative, sentences in tqdm(repr_sentences.items(), total=len(repr_sentences)):\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split(' ')\n",
    "            for idx, word in enumerate(sentence_words):\n",
    "                if word == representative:\n",
    "                    sentence_words[idx] = word.upper()\n",
    "            out_file.write(' '.join(sentence_words) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "with open('task3_polish_upper.txt', 'r') as in_file:\n",
    "    upper_sentences = in_file.read()\n",
    "upper_sentences = upper_sentences.split('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "representatives_upper = set([representative.upper() for representative in list(representatives)])\n",
    "repr_sentences = defaultdict(list)\n",
    "\n",
    "for sentence in upper_sentences:\n",
    "    sentence_words = set(sentence.split(' '))\n",
    "    for word in list(sentence_words & representatives_upper):\n",
    "        repr_sentences[word].append(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/836 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b11a95d92b446cda0e45fccc49e8e72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_upper_to_lower.txt', 'w') as out_file:\n",
    "    for representative, sentences in tqdm(repr_sentences.items(), total=len(repr_sentences)):\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split(' ')\n",
    "            for idx, word in enumerate(sentence_words):\n",
    "                if word == representative:\n",
    "                    sentence_words[idx] = word.lower()\n",
    "            out_file.write(' '.join(sentence_words) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "with open('task3_corpus.txt', 'w') as out_file:\n",
    "    for file in ['task3_polish_lower.txt', 'task3_polish_upper.txt', 'task3_lower_to_upper.txt', 'task3_upper_to_lower.txt']:\n",
    "        with open(file, 'r') as in_file:\n",
    "            shutil.copyfileobj(in_file, out_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task3_corpus.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "model.save('task3.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "upper_words = list(pd.Series(upper_sentences).apply(lambda sentence: str(sentence).split(' ')).explode().unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "upper_words = np.array([word for word in upper_words if len(word) > 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "representatives = list(representatives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def evaluate(word: str, model: Word2Vec):\n",
    "    distances = model.wv.distances(word, other_words=list(upper_words))\n",
    "    try:\n",
    "        position = np.argwhere(upper_words[np.argsort(distances)] == word.upper())[0][0]\n",
    "    except:\n",
    "        print(word)\n",
    "        return 0\n",
    "    return 1 / (position+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/836 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "205ffab6fce04f66ab56de324dd0bcc1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8958234206104013\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for word in tqdm(representatives):\n",
    "    scores.append(evaluate(word, model))\n",
    "\n",
    "print(np.mean(scores))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "lower_words = lower_word_counts.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "lower_words = np.array([word for word in lower_words if len(word) > 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55eac30c673241088c9f0b195e9d0311"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without\n",
      "leonida\n",
      "andriej\n",
      "satellite\n",
      "geschichte\n",
      "puerta\n",
      "plamisty\n",
      "sagem\n",
      "notecią\n",
      "okolo\n",
      "attack\n",
      "a5\n",
      "vel\n",
      "tkacz\n",
      "come\n",
      "being\n",
      "maurice\n",
      "survey\n",
      "does\n",
      "powiedzial\n",
      "risk\n",
      "change\n",
      "triple\n",
      "cid\n",
      "since\n",
      "carla\n",
      "duza\n",
      "highway\n",
      "antoni_antonim\n",
      "erwin\n",
      "0.1675091176190198\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for word in tqdm(np.random.choice(lower_words, 5000, replace=False)):\n",
    "    scores.append(evaluate(word, model))\n",
    "\n",
    "print(np.mean(scores))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "scores = []\n",
    "for word in D:\n",
    "    if word.islower():\n",
    "        scores.append(evaluate(word, model_corpus))\n",
    "print(np.array(scores).mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "4947e307",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 4 (4 points)\n",
    "\n",
    "In this task you are asked to do two things:\n",
    "1. compare the embeddings computed on small corpus (like Brown Corpus , see: <https://en.wikipedia.org/wiki/Brown_Corpus>) with the ones coming from Google News Corpus\n",
    "2. Try to use other resourses like WordNet to enrich to corpus, and obtain better embeddings\n",
    "\n",
    "You can use the following code snippets:\n",
    "\n",
    "```python\n",
    "# printing tokenized Brown Corpora\n",
    "from nltk.corpus import brown\n",
    "for s in brown.sents():\n",
    "    print(*s)\n",
    "    \n",
    "#iterating over all synsets in WordNet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
    "    for synset in list(wn.all_synsets(synset_type)))[:10]:\n",
    "        print (synset.definition())\n",
    "        print (synset.examples())\n",
    "        print ([lem.name() for lem in synset.lemmas()])\n",
    "        print (synset.hyperonims()) # nodes 1 level up in ontology\n",
    "        \n",
    "# loading model and compute cosine similarity between words\n",
    "\n",
    "model = Word2Vec.load('models/w2v.wordnet5.model') \n",
    "print (model.wv.similarity('dog', 'cat'))\n",
    "```\n",
    "\n",
    "Embeddings will be tested using WordSim-353 dataset, the code showing the quality is in the cell below. Prepare the following corpora:\n",
    "1. Tokenized Brown Corpora\n",
    "2. Definitions and examples from Princeton WordNet\n",
    "3. (1) and (2) together\n",
    "4. (3) enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')\n",
    "\n",
    "Train 4 Word2Vec models, and raport Spearman correletion between similarities based on your vectors, and similarities based on human judgements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a2fbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code for computing correlation between W2V similarity, and human judgements\n",
    "\n",
    "import gensim.downloader\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "gn = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "for similarity_type in ['relatedness', 'similarity']:\n",
    "    ws353 = []\n",
    "    for x in open(f'wordsim_{similarity_type}_goldstandard.txt'): \n",
    "        a,b,val = x.split()\n",
    "        val = float(val)\n",
    "        ws353.append( (a,b,val))\n",
    "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
    "    print (similarity_type + ':', spearmanr(vals, ys)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf71c95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}