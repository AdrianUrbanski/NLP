{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef34449",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assigment 4\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* get at least 4 points by Tuesday, 12.05.2022\n",
    "* remaining points: last lab session before or on Tuesday, 19.05.2022\n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1HaMbhzaBxxNa_z_QJXSDCbv5VddmhVVZ?usp=sharing> (or will be soon :) )\n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss from Slide 3 in Lecture NLP.2, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "data_pdf = pd.read_csv('task1_objects_contexts_polish.txt', sep=' ', names=['object', 'context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "object_encoder = LabelEncoder()\n",
    "data_pdf['object'] = object_encoder.fit_transform(data_pdf['object'])\n",
    "\n",
    "context_encoder = LabelEncoder()\n",
    "data_pdf['context'] = context_encoder.fit_transform(data_pdf['context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, data_pdf):\n",
    "        self.positive_samples = data_pdf.groupby('context').apply(lambda pdf: set(pdf['object'].unique()))\n",
    "        self.object_distribution = np.power(data_pdf['object'].value_counts(), 3/4)\n",
    "        self.all_samples = set(data_pdf['object'].unique())\n",
    "\n",
    "    def sample(self, context, k):\n",
    "        negative_samples = list(self.all_samples - self.positive_samples[context])\n",
    "        negative_distribution = self.object_distribution[negative_samples]\n",
    "        negative_distribution /= np.sum(negative_distribution)\n",
    "        return np.random.choice(negative_distribution.index, size=k, replace=False, p=negative_distribution.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def object_grad(object_embedding, context_embedding):\n",
    "    return -(1-sigmoid(np.dot(object_embedding, context_embedding)))*context_embedding\n",
    "\n",
    "def context_grad(object_embedding, context_embedding, negative_embeddings):\n",
    "    return (-(1-sigmoid(object_embedding@context_embedding))*object_embedding\n",
    "            + np.sum((1-sigmoid(-context_embedding.reshape(1, -1) @ negative_embeddings.T))*negative_embeddings.T, axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def loss_fun(object_embedding, context_embedding, negative_embeddings):\n",
    "    return (-np.log(sigmoid(object_embedding @ context_embedding))\n",
    "            - np.sum(sigmoid(-context_embedding.reshape(1, -1) @ negative_embeddings.T)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, lr: float, k: int, embeddings_size: int):\n",
    "        self.lr = lr\n",
    "        self.k = k\n",
    "        self.object_embeddings = np.random.rand(len(data_pdf['object'].unique()), embeddings_size)\n",
    "        self.context_embeddings = np.random.rand(len(data_pdf['context'].unique()), embeddings_size)\n",
    "\n",
    "    def step(self, object, context, samples):\n",
    "        negative_objects = np.array(samples)\n",
    "        self.object_embeddings[object] -= self.lr*object_grad(self.object_embeddings[object], self.context_embeddings[context])\n",
    "        self.context_embeddings[context] -= self.lr*context_grad(self.object_embeddings[object], self.context_embeddings[context], self.object_embeddings[negative_objects])\n",
    "        return loss_fun(self.object_embeddings[object], self.context_embeddings[context], self.object_embeddings[negative_objects])\n",
    "\n",
    "    def run_epoch(self, data, log_freq, distribution):\n",
    "        negative_samples = np.random.choice(distribution.index, p=distribution.values, size=(len(data), self.k))\n",
    "        iter = 0\n",
    "        loss = 0\n",
    "        for object, context in tqdm(data):\n",
    "            loss += self.step(object, context, negative_samples[iter])\n",
    "            iter += 1\n",
    "            if log_freq is not None and iter % log_freq == 0:\n",
    "                print('loss: ', loss/iter)\n",
    "\n",
    "    def fit(self, data_pdf, num_epochs, log_freq = 1000000):\n",
    "        distribution = np.power(data_pdf['object'].value_counts(), 3/4)\n",
    "        distribution /= sum(distribution)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(epoch)\n",
    "            self.run_epoch(data_pdf.values, log_freq, distribution)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c4cda658b0b4f93b93d2dc56d3746b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -0.976483554272937\n",
      "loss:  -1.3400992543096206\n",
      "loss:  -1.5321450409319193\n",
      "loss:  -1.658324154624081\n",
      "loss:  -1.7505253612235852\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c633381c25b4a76a63c3e7641e50dd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.24171407203101\n",
      "loss:  -2.270901648276874\n",
      "loss:  -2.2904629613822967\n",
      "loss:  -2.3048811849044286\n",
      "loss:  -2.315929483039553\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4388847fc9464c81ba1b35ff738f2940"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3927689334846476\n",
      "loss:  -2.4005408104554\n",
      "loss:  -2.403134796950425\n",
      "loss:  -2.4043327410947035\n",
      "loss:  -2.404318121539505\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53f723d35e46461698897aa8157f9a59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.4091351594575796\n",
      "loss:  -2.4091184871845384\n",
      "loss:  -2.40660682144957\n",
      "loss:  -2.403637415284451\n",
      "loss:  -2.4004625914182163\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ca36571f20545abbf4a458087207360"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.38529273200978\n",
      "loss:  -2.384298887248675\n",
      "loss:  -2.3816417914161003\n",
      "loss:  -2.379505673691447\n",
      "loss:  -2.376775936987627\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b0779ddf5914644be240c4d2494b7c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.365745142543159\n",
      "loss:  -2.365264209055516\n",
      "loss:  -2.362773496095009\n",
      "loss:  -2.3605348128264025\n",
      "loss:  -2.3581710022408773\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d7770a5c7284abeb48a81c8a5f5a962"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.351082214021668\n",
      "loss:  -2.350176746681713\n",
      "loss:  -2.3482948966123516\n",
      "loss:  -2.347176325622098\n",
      "loss:  -2.3455854880115394\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2c2c6df275e4d6e845a65958f7f4904"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.341554906687945\n",
      "loss:  -2.341195241428283\n",
      "loss:  -2.3401067604251096\n",
      "loss:  -2.339340450009273\n",
      "loss:  -2.3385643535662677\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd5c561ea5a8487b85916db67619f85d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3378953289502684\n",
      "loss:  -2.337793450528365\n",
      "loss:  -2.337329117158638\n",
      "loss:  -2.3373217386999716\n",
      "loss:  -2.3368739804082472\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66c630a2a4514fd0964245e675dbb3ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3382983613315256\n",
      "loss:  -2.3388867824362958\n",
      "loss:  -2.3384812736516216\n",
      "loss:  -2.3383699275041407\n",
      "loss:  -2.338142029682852\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a8e99c851834d1287319f657065e827"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.342033898324023\n",
      "loss:  -2.3426512288061803\n",
      "loss:  -2.342438706678854\n",
      "loss:  -2.342877583124321\n",
      "loss:  -2.3430062163415735\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "561691a4daf74251a10b854c6c1e0723"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3476531092380117\n",
      "loss:  -2.3484678671646053\n",
      "loss:  -2.3486585503966606\n",
      "loss:  -2.349268881553964\n",
      "loss:  -2.3496627214495716\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22b626c523474b19b24ff6672c8413cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3566118121149886\n",
      "loss:  -2.3572747522531126\n",
      "loss:  -2.3576562897153446\n",
      "loss:  -2.3580801144250216\n",
      "loss:  -2.358673227115826\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f81923cd5464d40aa99d4e97f560b78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.365580979058521\n",
      "loss:  -2.3662356200597117\n",
      "loss:  -2.367095737430774\n",
      "loss:  -2.3678132062664488\n",
      "loss:  -2.36843804281139\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5b532f666744b9d8fc5b7995e92aa98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.376241827862562\n",
      "loss:  -2.377153089262103\n",
      "loss:  -2.377807766103135\n",
      "loss:  -2.3786442734349085\n",
      "loss:  -2.379277821616192\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169c231c16a64523ba23aec1c455f401"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3863342654820494\n",
      "loss:  -2.387491426237623\n",
      "loss:  -2.388458916139939\n",
      "loss:  -2.3894602838052874\n",
      "loss:  -2.390311416760087\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b00378c25f1e4d4e80831d72cf3e68bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3975970855434103\n",
      "loss:  -2.398661025792496\n",
      "loss:  -2.3995538291099727\n",
      "loss:  -2.400741819020991\n",
      "loss:  -2.4016317781935426\n",
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e748f36b59154218bfc2d876655cb2ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.410046844944847\n",
      "loss:  -2.4109064001713154\n",
      "loss:  -2.4117589725494937\n",
      "loss:  -2.412471475428429\n",
      "loss:  -2.413321914258733\n",
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b86fe66a3d294641a40f0ee654f1eb82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.4214263697881115\n",
      "loss:  -2.421966135179359\n",
      "loss:  -2.4233579476737837\n",
      "loss:  -2.424311271507467\n",
      "loss:  -2.425219977958236\n",
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e142b19065574bc1a01633452985df31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.432611052232172\n",
      "loss:  -2.433632762885281\n",
      "loss:  -2.4350643908244107\n",
      "loss:  -2.4361109515654737\n",
      "loss:  -2.437166534018982\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(0.01, 5, 100)\n",
    "word2vec.fit(data_pdf, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "embeddings = word2vec.object_embeddings\n",
    "embeddings_pdf = pd.DataFrame(embeddings)\n",
    "embeddings_pdf.index = object_encoder.inverse_transform(embeddings_pdf.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "with open('task1_w2v_vectors.txt', 'w') as out_file:\n",
    "    out_file.write(f'{len(embeddings_pdf)} {len(embeddings_pdf.columns)}\\n')\n",
    "    embeddings_pdf.to_csv(out_file, header=False, index=True, sep=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ae909d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    kot 0.8010485768318176\n",
      "    koń 0.7975372076034546\n",
      "    zwierzę 0.7651735544204712\n",
      "    chłopiec 0.7597785592079163\n",
      "    dziewczyna 0.7183960676193237\n",
      "    mężczyzna 0.7060914635658264\n",
      "    chłopak 0.700786828994751\n",
      "    dziewczynka 0.6936959028244019\n",
      "    ptak 0.6886632442474365\n",
      "    kobieta 0.6841621398925781\n",
      "\n",
      "WORD: smok\n",
      "    niedźwiedź 0.6620179414749146\n",
      "    bocian 0.6596109867095947\n",
      "    bestia 0.6152178049087524\n",
      "    dzieciak 0.6127355098724365\n",
      "    ptak 0.6104136109352112\n",
      "    potwór 0.6066800355911255\n",
      "    tygrys 0.6056229472160339\n",
      "    słoń 0.6001629829406738\n",
      "    wilk 0.5967732667922974\n",
      "    wąż 0.5922409296035767\n",
      "\n",
      "WORD: miłość\n",
      "    wiara 0.7513876557350159\n",
      "    duch 0.6542425751686096\n",
      "    uczucie 0.6329718232154846\n",
      "    śmierć 0.627207338809967\n",
      "    natura 0.6237786412239075\n",
      "    młodość 0.6211560964584351\n",
      "    wolność 0.6175273656845093\n",
      "    radość 0.6165459752082825\n",
      "    miłosierdzie 0.6136339902877808\n",
      "    wyobraźnia 0.6124131083488464\n",
      "\n",
      "WORD: rower\n",
      "    auto 0.6595624089241028\n",
      "    wózek 0.6472136378288269\n",
      "    ciężarówka 0.6223849058151245\n",
      "    torba 0.6204795241355896\n",
      "    plecak 0.6048317551612854\n",
      "    motocykl 0.6007189154624939\n",
      "    rakieta 0.5908415913581848\n",
      "    pociąg 0.5879620313644409\n",
      "    paczka 0.5861489772796631\n",
      "    autobus 0.5833216905593872\n",
      "\n",
      "WORD: maraton\n",
      "    rozgrywka 0.5802261829376221\n",
      "    piknik 0.5518872737884521\n",
      "    rajd 0.5512055158615112\n",
      "    wyścig 0.5315948724746704\n",
      "    sesja 0.5302449464797974\n",
      "    zmaganie 0.5217149257659912\n",
      "    turniej 0.5203548669815063\n",
      "    mecz 0.5055239200592041\n",
      "    finał 0.4991457164287567\n",
      "    potyczka 0.4986191391944885\n",
      "\n",
      "WORD: logika\n",
      "    swoboda 0.6528512835502625\n",
      "    prostota 0.6490967869758606\n",
      "    spójność 0.6462158560752869\n",
      "    złożoność 0.6453420519828796\n",
      "    efektywność 0.6338891386985779\n",
      "    specyfika 0.6332631707191467\n",
      "    estetyka 0.630141019821167\n",
      "    uczciwość 0.6301333904266357\n",
      "    powaga 0.6250840425491333\n",
      "    motywacja 0.6248546838760376\n",
      "\n",
      "WORD: motyl\n",
      "    ptak 0.6189852356910706\n",
      "    świnia 0.5453046560287476\n",
      "    potwór 0.544842004776001\n",
      "    smok 0.5391629338264465\n",
      "    pszczoła 0.5320708751678467\n",
      "    owad 0.5124348402023315\n",
      "    jajo 0.5077357292175293\n",
      "    szczur 0.5058262348175049\n",
      "    bomba 0.5008795261383057\n",
      "    stop 0.499335378408432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors.txt', binary=False)\n",
    "\n",
    "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
    "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "example_words = example_polish_words\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task1_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2 (4 points)\n",
    "\n",
    "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
    "\n",
    "```python\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "```\n",
    "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file] containing such pairs of titles, that one article links to another.\n",
    "\n",
    "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
    "\n",
    "Compare these two approaches using similar code to the code from Task 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6ad8e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wiki_links_pdf = pd.read_csv('task2_simple.wiki.links.txt', sep=' ', names=['object', 'context'])\n",
    "# The cell for your presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "wiki_titles = set(np.hstack((wiki_links_pdf['object'], wiki_links_pdf['context'])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "word_counts = pd.Series(list(wiki_titles)).apply(lambda title: str(title).split('_')).explode().value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "rare_words = word_counts[(word_counts > 1) & (word_counts <= 30)].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79ac7e430df242c588ae4f18a1a2c4f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles = defaultdict(list)\n",
    "wiki_titles = list(wiki_titles)\n",
    "rare_words = set(rare_words)\n",
    "\n",
    "for it, title in tqdm(enumerate(wiki_titles)):\n",
    "    for word in list(set(str(title).split('_')) & rare_words):\n",
    "        articles[word].append(it)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/204926 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb0c898d45b840e49fa50c51d5c27b29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task2_related_wiki_links.txt', 'w') as out_file:\n",
    "    for vals in tqdm(articles.values()):\n",
    "        for pair in combinations(vals, 2):\n",
    "            out_file.write(f'{wiki_titles[pair[0]]} {wiki_titles[pair[1]]}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "example_titles = [\n",
    "    'capital_city',\n",
    "    'flower',\n",
    "    'mickey_mouse',\n",
    "    'finance',\n",
    "    'japan_national_under-23_football_team',\n",
    "    'john_the_apostle',\n",
    "    'boss_(gaming)',\n",
    "    'the_wealth_of_nations',\n",
    "    'sulfur_dioxide',\n",
    "    'tandem_bicycle',\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_simple.wiki.links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: capital_city\n",
      "    population_density 0.739251434803009\n",
      "    province 0.7069327235221863\n",
      "    plain 0.701899528503418\n",
      "    oceanic_climate 0.6916329264640808\n",
      "    köppen_climate_classification 0.6897584199905396\n",
      "    list_of_capital_cities_by_altitude 0.6814950704574585\n",
      "    category:departments_of_paraguay 0.6782993674278259\n",
      "    above_sea_level 0.6735855340957642\n",
      "    category:departments_of_the_republic_of_the_congo 0.6711230874061584\n",
      "    hill 0.669350266456604\n",
      "\n",
      "WORD: flower\n",
      "    asteraceae 0.8791819214820862\n",
      "    leaf 0.8778598308563232\n",
      "    berry 0.8747152090072632\n",
      "    vine 0.8712254166603088\n",
      "    seed 0.8700944781303406\n",
      "    perennial 0.8616876602172852\n",
      "    tail 0.8591261506080627\n",
      "    fruit 0.856979489326477\n",
      "    sweet_(taste) 0.8559210300445557\n",
      "    lime 0.8551871180534363\n",
      "\n",
      "WORD: mickey_mouse\n",
      "    minnie_mouse 0.9010185599327087\n",
      "    goofy 0.8845751285552979\n",
      "    bugs_bunny 0.8688761591911316\n",
      "    donald_duck 0.8686318397521973\n",
      "    daffy_duck 0.857617974281311\n",
      "    pluto_(disney) 0.8547376990318298\n",
      "    walt_disney_company 0.8538717031478882\n",
      "    tokyo_disneyland 0.8529824614524841\n",
      "    daisy_duck 0.8525812029838562\n",
      "    fred_moore_(animator) 0.8519846796989441\n",
      "\n",
      "WORD: finance\n",
      "    investment 0.859107494354248\n",
      "    banking 0.8380959630012512\n",
      "    insurance 0.828545868396759\n",
      "    stock_market 0.8262161016464233\n",
      "    financial 0.819246232509613\n",
      "    auction 0.8171119689941406\n",
      "    shareholder 0.8169492483139038\n",
      "    business 0.8091278076171875\n",
      "    share_(finance) 0.8073405623435974\n",
      "    wikt:found 0.8066060543060303\n",
      "\n",
      "WORD: japan_national_under-23_football_team\n",
      "    hong_kong_national_football_team 0.9604832530021667\n",
      "    2011_afc_asian_cup 0.9599856734275818\n",
      "    football_at_the_1966_asian_games 0.9528710246086121\n",
      "    2019_afc_asian_cup 0.951862633228302\n",
      "    football_at_the_1970_asian_games 0.9510948061943054\n",
      "    1988_afc_asian_cup 0.9494122862815857\n",
      "    football_at_the_1986_asian_games 0.9492978453636169\n",
      "    2000_afc_asian_cup 0.948467493057251\n",
      "    football_at_the_2002_asian_games_–_men's_tournament 0.9473084211349487\n",
      "    football_at_the_1978_asian_games 0.9464484453201294\n",
      "\n",
      "WORD: john_the_apostle\n",
      "    gospel_of_mark 0.978923499584198\n",
      "    acts_of_the_apostles 0.9740555286407471\n",
      "    matthew_the_evangelist 0.9712485671043396\n",
      "    jerome 0.9652677774429321\n",
      "    category:ancient_christianity 0.9603877663612366\n",
      "    theotokos 0.960051417350769\n",
      "    nicene_creed 0.9586400389671326\n",
      "    gospel_of_luke 0.9579897522926331\n",
      "    origen 0.9572450518608093\n",
      "    paul_of_tarsus 0.9551813006401062\n",
      "\n",
      "WORD: boss_(gaming)\n",
      "    nascar_07 0.9553720951080322\n",
      "    list_of_acclaim_entertainment_subsidiaries#acclaim_studios_cheltenham 0.9435751438140869\n",
      "    onlive 0.9433116912841797\n",
      "    british_academy_games_awards 0.942517876625061\n",
      "    megatech 0.9415945410728455\n",
      "    conficker_a 0.9402057528495789\n",
      "    barney_calhoun 0.9397355914115906\n",
      "    cs_go 0.9394882321357727\n",
      "    barbie_horse_adventures_ii:_riding_camp 0.9381359815597534\n",
      "    14_degrees_east 0.937997579574585\n",
      "\n",
      "WORD: the_wealth_of_nations\n",
      "    ricardian_model 0.952191174030304\n",
      "    neoclassical_economics 0.9482550621032715\n",
      "    the_theory_of_moral_sentiments 0.9475444555282593\n",
      "    classical_economics 0.9471796751022339\n",
      "    category:anarchism 0.943265438079834\n",
      "    economies_of_scale 0.9426497220993042\n",
      "    public_good 0.9414759874343872\n",
      "    category:economic_theories 0.9397423267364502\n",
      "    category:economic_systems 0.9373308420181274\n",
      "    open_market 0.9350383281707764\n",
      "\n",
      "WORD: sulfur_dioxide\n",
      "    crystallization 0.9755793213844299\n",
      "    sulfuric_acid 0.9725997447967529\n",
      "    magnetite 0.9715259075164795\n",
      "    specific_gravity 0.9689416289329529\n",
      "    malleable 0.967831552028656\n",
      "    sodium_chloride 0.9677532911300659\n",
      "    titanium_dioxide 0.967487633228302\n",
      "    uranyl_nitrate 0.9673746824264526\n",
      "    carbonic_acid 0.966763436794281\n",
      "    carbon_steel 0.9667571187019348\n",
      "\n",
      "WORD: tandem_bicycle\n",
      "    elena_marsili 0.9514120221138\n",
      "    crown_prince_of_greece 0.95046466588974\n",
      "    category:tunisian_producers 0.9477298259735107\n",
      "    jerzy_kowalczyk 0.9473161101341248\n",
      "    category:belgian_atheists 0.9470301866531372\n",
      "    josé_agustín_goytisolo 0.9466261267662048\n",
      "    category:guyanese_writers 0.9465793371200562\n",
      "    swimming_at_the_1992_summer_paralympics 0.9458359479904175\n",
      "    premio_alfaguara 0.9450024962425232\n",
      "    mingo_y_aníbal_contra_los_fantasmas 0.9446794986724854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_simple_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task2_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_related_wiki_links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: capital_city\n",
      "capital_city not found in the training set\n",
      "\n",
      "WORD: flower\n",
      "flower not found in the training set\n",
      "\n",
      "WORD: mickey_mouse\n",
      "mickey_mouse not found in the training set\n",
      "\n",
      "WORD: finance\n",
      "finance not found in the training set\n",
      "\n",
      "WORD: japan_national_under-23_football_team\n",
      "japan_national_under-23_football_team not found in the training set\n",
      "\n",
      "WORD: john_the_apostle\n",
      "john_the_apostle not found in the training set\n",
      "\n",
      "WORD: boss_(gaming)\n",
      "    first-person_(gaming) 0.974440336227417\n",
      "    item_(gaming) 0.97352534532547\n",
      "    match_(gaming) 0.9721232056617737\n",
      "    ken_williams_(gaming) 0.971487283706665\n",
      "    deathmatch_(gaming) 0.9703531861305237\n",
      "    spam_(gaming) 0.966529130935669\n",
      "    kana_kitahara 0.9655419588088989\n",
      "    stalling_(gaming) 0.9654951691627502\n",
      "    first_person_(gaming) 0.9649636149406433\n",
      "    toa_alta,_puerto_rico 0.9643204212188721\n",
      "\n",
      "WORD: the_wealth_of_nations\n",
      "    wealth_management 0.9905750751495361\n",
      "    morgan_stanley_wealth_management 0.9884107112884521\n",
      "    list_of_countries_by_distribution_of_wealth 0.9877914190292358\n",
      "    appeal_to_wealth 0.9875590801239014\n",
      "    wealth_(movie) 0.9873790144920349\n",
      "    gross_domestic_product#standard_of_living_and_gdp:_wealth_distribution_and_externalities 0.9867001175880432\n",
      "    sovereign_wealth_fund 0.9865601658821106\n",
      "    evercore_wealth_management 0.9864317178726196\n",
      "    wealth_gap_in_the_united_states 0.986093282699585\n",
      "    wealth_redistribution 0.9860005378723145\n",
      "\n",
      "WORD: sulfur_dioxide\n",
      "    image:mauna_loa_sulfur_hexafluoride_concentration.jpg 0.9789128303527832\n",
      "    sulfur_trioxide 0.9754117131233215\n",
      "    rhenium_dioxide 0.974718451499939\n",
      "    bromine_dioxide 0.9737715125083923\n",
      "    sulfur_pentafluoride 0.973708987236023\n",
      "    sulfur_monoxide 0.9730807542800903\n",
      "    selenium_dioxide 0.9724143743515015\n",
      "    sulfur_dichloride 0.9720863103866577\n",
      "    sulfur_nitride 0.9719774127006531\n",
      "    sulfur_tetrachloride 0.97173672914505\n",
      "\n",
      "WORD: tandem_bicycle\n",
      "    amino_acids 0.9693626165390015\n",
      "    argonne_tandem_linear_accelerator_system 0.9685113430023193\n",
      "    tandem_repeat 0.9683820605278015\n",
      "    aren't_you_glad_you're_you? 0.9681868553161621\n",
      "    akari_inaba 0.9663731455802917\n",
      "    retro_vhs_blu-ray 0.9661436676979065\n",
      "    recommended_citation:_frost,_darrel_r._2019._amphibian_species_of_the_world:_an_online_reference._version_6.0_(date_of_access)._electronic_database_accessible_at_http://research.amnh.org/herpetology/amphibia/index.html._american_museum_of_natural_history,_new_york,_usa. 0.9659613966941833\n",
      "    file:us_navy_070412-n-7498l-018_a_french_super-etendard_from_the_nuclear-powered_aircraft_carrier_french_navy_ship_charles_de_gaulle_(r_91)_performs_a_touch-and-go_landing_on_the_flight_deck_of_the_nimitz-class_aircraft_carrier_uss.jpg 0.965947687625885\n",
      "    bitlis_vilayet 0.965805172920227\n",
      "    drying_rack 0.9657403826713562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_related_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    try:\n",
    "        for w, v in task2_wv.most_similar(w):\n",
    "            print ('   ', w, v)\n",
    "    except:\n",
    "        print(f'{w} not found in the training set')\n",
    "    print ()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import shutil\n",
    "with open('task2_combined_wiki_links.txt', 'wb') as out_file:\n",
    "    for file in ['task2_related_wiki_links.txt', 'task2_simple.wiki.links.txt' ]:\n",
    "        with open(file, 'rb') as in_file:\n",
    "            shutil.copyfileobj(in_file, out_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_combined_wiki_links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: capital_city\n",
      "    list_of_national_capitals 0.7547511458396912\n",
      "    population_density 0.7497968077659607\n",
      "    list_of_capital_cities_by_altitude 0.745806872844696\n",
      "    province 0.727108359336853\n",
      "    köppen_climate_classification 0.7214742302894592\n",
      "    communes_of_burundi 0.7195709943771362\n",
      "    port 0.712222158908844\n",
      "    hill 0.7095301151275635\n",
      "    category:regions_of_burkina_faso 0.7092888355255127\n",
      "    coast 0.7067225575447083\n",
      "\n",
      "WORD: flower\n",
      "    berry 0.9138245582580566\n",
      "    evergreen 0.91241455078125\n",
      "    asteraceae 0.9113523364067078\n",
      "    leaf 0.9072189927101135\n",
      "    flowers 0.8990909457206726\n",
      "    sweet_(taste) 0.8957048654556274\n",
      "    buckwheat 0.8947560787200928\n",
      "    fruit 0.8932700157165527\n",
      "    orchidaceae 0.8918755054473877\n",
      "    shrub 0.8892049193382263\n",
      "\n",
      "WORD: mickey_mouse\n",
      "    donald_duck 0.9323548078536987\n",
      "    pluto_(disney) 0.9035681486129761\n",
      "    american_pekin_duck 0.8992806673049927\n",
      "    money_bin 0.8984423279762268\n",
      "    goofy 0.8974982500076294\n",
      "    daisy_duck 0.8972910642623901\n",
      "    category:mickey_mouse_universe_characters 0.8919526934623718\n",
      "    huey,_dewey_and_louie 0.8915558457374573\n",
      "    super_ducktales 0.8892890214920044\n",
      "    fred_moore_(animator) 0.8882454037666321\n",
      "\n",
      "WORD: finance\n",
      "    investment 0.879478931427002\n",
      "    insurance 0.8725610375404358\n",
      "    shareholder 0.8627383708953857\n",
      "    infrastructure 0.8478735089302063\n",
      "    share_(finance) 0.8476279377937317\n",
      "    non-profit 0.8457669019699097\n",
      "    category:commerce 0.8376730680465698\n",
      "    business_school 0.8371889591217041\n",
      "    junior_college 0.8369675278663635\n",
      "    business 0.8355017304420471\n",
      "\n",
      "WORD: japan_national_under-23_football_team\n",
      "    hong_kong_national_football_team 0.9595895409584045\n",
      "    2000_afc_asian_cup 0.956436276435852\n",
      "    2011_afc_asian_cup 0.954825222492218\n",
      "    football_at_the_2010_asian_games_–_men's_tournament 0.9512404799461365\n",
      "    football_at_the_2002_asian_games_–_men's_tournament 0.9485815167427063\n",
      "    football_at_the_2018_asian_games_–_men's_tournament 0.947322428226471\n",
      "    football_at_the_1986_asian_games 0.9472708702087402\n",
      "    football_at_the_1966_asian_games 0.9455299973487854\n",
      "    1996_afc_asian_cup 0.9451062083244324\n",
      "    uae_pro_league 0.9449082612991333\n",
      "\n",
      "WORD: john_the_apostle\n",
      "    gospel_of_mark 0.9818589687347412\n",
      "    theotokos 0.9703783988952637\n",
      "    paul_the_apostle 0.9666560888290405\n",
      "    deuterocanonical 0.964203417301178\n",
      "    transfiguration 0.9638434052467346\n",
      "    christology 0.9635299444198608\n",
      "    book_of_odes_(bible) 0.9619091749191284\n",
      "    development_of_the_new_testament_canon 0.9609437584877014\n",
      "    origen 0.960762619972229\n",
      "    one_holy_catholic_and_apostolic_church 0.9605040550231934\n",
      "\n",
      "WORD: boss_(gaming)\n",
      "    item_(gaming) 0.9903486371040344\n",
      "    unlockable_(gaming) 0.9896479249000549\n",
      "    health_(gaming) 0.9893690943717957\n",
      "    quest_(gaming) 0.9889283180236816\n",
      "    life_(gaming) 0.9881860017776489\n",
      "    mod_(gaming) 0.9881646633148193\n",
      "    match_(gaming) 0.9864320755004883\n",
      "    ken_williams_(gaming) 0.9851617813110352\n",
      "    first_person_(gaming) 0.9826708436012268\n",
      "    game_(simulation) 0.9797883033752441\n",
      "\n",
      "WORD: the_wealth_of_nations\n",
      "    category:people_by_wealth_or_status 0.9434105753898621\n",
      "    national_wealth 0.8976672887802124\n",
      "    redistribution_of_income_and_wealth 0.8891507983207703\n",
      "    gross_domestic_product#standard_of_living_and_gdp:_wealth_distribution_and_externalities 0.8879180550575256\n",
      "    net_wealth 0.8850098848342896\n",
      "    wealth_redistribution 0.8792318105697632\n",
      "    objectivism_(ayn_rand) 0.8792073130607605\n",
      "    the_black_book:_imbalance_of_power_and_wealth_in_the_sudan 0.8788139224052429\n",
      "    category:society 0.8767331838607788\n",
      "    powershift:_knowledge,_wealth_and_violence_at_the_edge_of_the_21st_century 0.874108076095581\n",
      "\n",
      "WORD: sulfur_dioxide\n",
      "    oxide 0.9689789414405823\n",
      "    sulfate 0.9675155878067017\n",
      "    sulfuric_acid 0.9640898108482361\n",
      "    crystalline 0.9618420600891113\n",
      "    hydroxide 0.9603775143623352\n",
      "    iron(ii)_sulfate 0.95839923620224\n",
      "    sodium 0.9582377076148987\n",
      "    iron(iii)_oxide 0.9580594897270203\n",
      "    sodium_chloride 0.9577062726020813\n",
      "    template:chemical_formula/atom 0.9575254917144775\n",
      "\n",
      "WORD: tandem_bicycle\n",
      "    cycling_at_the_2014_commonwealth_games_–_men's_tandem_1km_time_trial_b 0.9946358799934387\n",
      "    cycling_at_the_2014_commonwealth_games_–_men's_tandem_sprint_b 0.9927564859390259\n",
      "    tandem 0.9853826761245728\n",
      "    cycling_at_the_1964_summer_olympics_–_men's_tandem 0.9811911582946777\n",
      "    air_command_tandem 0.9800781607627869\n",
      "    independence_speed_tandem 0.9800019860267639\n",
      "    tandem_solar_cell 0.9799981713294983\n",
      "    harry_boot 0.9638161063194275\n",
      "    it:valvola_termoionica#triodo 0.9607505202293396\n",
      "    short_division 0.959282636642456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_combined_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print('WORD:', w)\n",
    "    try:\n",
    "        for w, v in task2_wv.most_similar(w):\n",
    "            print('   ', w, v)\n",
    "    except:\n",
    "        print(f'{w} not found in the training set')\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "d711fdca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 3 (4 points)\n",
    "\n",
    "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
    "\n",
    "<pre>\n",
    "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "</pre>\n",
    "\n",
    "And this is its translation into Lower:\n",
    "\n",
    "<pre>\n",
    "the quick brown fox jumps over the lazy dog\n",
    "</pre>\n",
    "\n",
    "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
    "\n",
    "Prepare the corpora wich contains three kind of sentences:\n",
    "* Upper corpus sentences\n",
    "* Lower corpus sentences\n",
    "* sentences derived from Upper/Lower corpus, modified using D\n",
    "\n",
    "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
    "\n",
    "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
    "\n",
    "<pre>\n",
    "WOLF, CAT, WOLVES, LION, gopher, dog\n",
    "</pre>\n",
    "\n",
    "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('task3_polish_lower.txt', 'r') as in_file:\n",
    "    lower_sentences = in_file.read()\n",
    "\n",
    "lower_sentences = lower_sentences.split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500001 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69f4af8aa61b41629ae07d5e43dcb837"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_polish_lower_clean.txt', 'w') as out_file:\n",
    "    for sentence in tqdm(lower_sentences):\n",
    "        clean_words = []\n",
    "        for word in sentence.split(' '):\n",
    "            if word.isalpha():\n",
    "                clean_words.append(word)\n",
    "        out_file.write(' '.join(clean_words) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500001 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5e06dec843e4ca9a1e7fabd09018574"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_polish_upper.txt', 'r') as in_file:\n",
    "    upper_sentences = in_file.read()\n",
    "\n",
    "upper_sentences = upper_sentences.split('\\n')\n",
    "\n",
    "with open('task3_polish_upper_clean.txt', 'w') as out_file:\n",
    "    for sentence in tqdm(upper_sentences):\n",
    "        clean_words = []\n",
    "        for word in sentence.split(' '):\n",
    "            if word.isalpha():\n",
    "                clean_words.append(word)\n",
    "        out_file.write(' '.join(clean_words) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open('task3_polish_lower_clean.txt', 'r') as in_file:\n",
    "    lower_sentences = in_file.read()\n",
    "\n",
    "lower_sentences = lower_sentences.split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lower_word_counts = pd.Series(lower_sentences).apply(lambda sentence: str(sentence).split(' ')).explode().value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task3_polish_lower_clean.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    kot 0.7162494659423828\n",
      "    koń 0.6585080027580261\n",
      "    dzieciak 0.633281409740448\n",
      "    ubranie 0.6104938387870789\n",
      "    szczeniak 0.5985265970230103\n",
      "    zwierzę 0.5964903235435486\n",
      "    facet 0.5896682143211365\n",
      "    tygrys 0.5828161835670471\n",
      "    królik 0.5826810598373413\n",
      "    dziewczyna 0.582609236240387\n",
      "\n",
      "WORD: smok\n",
      "    szatan 0.740702748298645\n",
      "    młodzieniec 0.7297379374504089\n",
      "    królewna 0.6882060766220093\n",
      "    czar 0.6866880655288696\n",
      "    troll 0.6853764057159424\n",
      "    nagi 0.6830270290374756\n",
      "    anioł 0.6823969483375549\n",
      "    babka 0.6780027747154236\n",
      "    chusta 0.676275908946991\n",
      "    płomień 0.6760099530220032\n",
      "\n",
      "WORD: miłość\n",
      "    bóg 0.7756487131118774\n",
      "    dusza 0.7062867879867554\n",
      "    bliźni 0.6815989017486572\n",
      "    dobroć 0.6799430251121521\n",
      "    zbawienie 0.6663631200790405\n",
      "    ideał 0.6582024693489075\n",
      "    miłosierdzie 0.6530075669288635\n",
      "    sympatia 0.6488829255104065\n",
      "    wiara 0.6486445665359497\n",
      "    pragnienie 0.6375133991241455\n",
      "\n",
      "WORD: rower\n",
      "    motocykl 0.7252520322799683\n",
      "    pieszy 0.6698290109634399\n",
      "    rolka 0.6598889231681824\n",
      "    rowerowy 0.6586746573448181\n",
      "    leżak 0.6528128981590271\n",
      "    samochód 0.6404153108596802\n",
      "    autobus 0.6388004422187805\n",
      "    wyprzedzanie 0.6325308084487915\n",
      "    wózek 0.6286035776138306\n",
      "    motorower 0.6260062456130981\n",
      "\n",
      "WORD: maraton\n",
      "    rajd 0.7282789349555969\n",
      "    smackdown 0.7163883447647095\n",
      "    zmaganie 0.697551965713501\n",
      "    sztafeta 0.6880622506141663\n",
      "    sprint 0.6838608384132385\n",
      "    dakar 0.6764237284660339\n",
      "    mś 0.6723265051841736\n",
      "    wyprawa 0.6706942319869995\n",
      "    weekend 0.6702665686607361\n",
      "    wyścig 0.6693032383918762\n",
      "\n",
      "WORD: logika\n",
      "    rozumowanie 0.5377861857414246\n",
      "    idea 0.5155345797538757\n",
      "    filozofia 0.5080944895744324\n",
      "    aksjologiczny 0.4661611020565033\n",
      "    odczucie 0.46585163474082947\n",
      "    myślenie 0.46373894810676575\n",
      "    doktryna 0.46345412731170654\n",
      "    jasno 0.46208181977272034\n",
      "    argumentacja 0.4611908197402954\n",
      "    pogląd 0.4532732665538788\n",
      "\n",
      "WORD: motyl\n",
      "    hara 0.7238243818283081\n",
      "    samba 0.7127453684806824\n",
      "    donieck 0.7082629799842834\n",
      "    beer 0.7067659497261047\n",
      "    uzbecki 0.7057417631149292\n",
      "    berger 0.7056270241737366\n",
      "    wiedźma 0.7002882957458496\n",
      "    padlina 0.6995993852615356\n",
      "    wonder 0.698060154914856\n",
      "    felicjan 0.6979899406433105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task3_wv = model.wv\n",
    "\n",
    "example_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task3_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "lower_kmeans = KMeans(n_clusters=100)\n",
    "preds = lower_kmeans.fit_predict(task3_wv.vectors)\n",
    "\n",
    "preds_pdf = pd.DataFrame(preds, columns=['cluster'])\n",
    "preds_pdf.index = task3_wv.index_to_key\n",
    "preds_pdf = preds_pdf.join(lower_word_counts.rename('count'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "cluster_representatives_pdf = preds_pdf.groupby('cluster').apply(lambda pdf: pdf.sort_values('count', ascending=False).iloc[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                    cluster   count\ncluster                            \n0       ten               0  184281\n        że                0   69068\n        który             0   63819\n        czy               0   25341\n        jak               0   25201\n...                     ...     ...\n99      raport           99     930\n        zapytanie        99     894\n        prośba           99     870\n        deklaracja       99     736\n        skarga           99     626\n\n[987 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>cluster</th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>cluster</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>ten</th>\n      <td>0</td>\n      <td>184281</td>\n    </tr>\n    <tr>\n      <th>że</th>\n      <td>0</td>\n      <td>69068</td>\n    </tr>\n    <tr>\n      <th>który</th>\n      <td>0</td>\n      <td>63819</td>\n    </tr>\n    <tr>\n      <th>czy</th>\n      <td>0</td>\n      <td>25341</td>\n    </tr>\n    <tr>\n      <th>jak</th>\n      <td>0</td>\n      <td>25201</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">99</th>\n      <th>raport</th>\n      <td>99</td>\n      <td>930</td>\n    </tr>\n    <tr>\n      <th>zapytanie</th>\n      <td>99</td>\n      <td>894</td>\n    </tr>\n    <tr>\n      <th>prośba</th>\n      <td>99</td>\n      <td>870</td>\n    </tr>\n    <tr>\n      <th>deklaracja</th>\n      <td>99</td>\n      <td>736</td>\n    </tr>\n    <tr>\n      <th>skarga</th>\n      <td>99</td>\n      <td>626</td>\n    </tr>\n  </tbody>\n</table>\n<p>987 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_representatives_pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "representatives = [idx[1] for idx in cluster_representatives_pdf.index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "with open('task3_representatives.txt', 'w') as out_file:\n",
    "    out_file.write(' '.join(representatives))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "repr_sentences = defaultdict(list)\n",
    "representatives = set(representatives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "for sentence in lower_sentences:\n",
    "    sentence_words = set(sentence.split(' '))\n",
    "    for word in list(sentence_words & representatives):\n",
    "        repr_sentences[word].append(sentence)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/987 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "702e0edf11154d8bab9fdc66cdd62c42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_lower_to_upper.txt', 'w') as out_file:\n",
    "    for representative, sentences in tqdm(repr_sentences.items(), total=len(repr_sentences)):\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split(' ')\n",
    "            for idx, word in enumerate(sentence_words):\n",
    "                if word == representative:\n",
    "                    sentence_words[idx] = word.upper()\n",
    "            out_file.write(' '.join(sentence_words) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "with open('task3_polish_upper_clean.txt', 'r') as in_file:\n",
    "    upper_sentences = in_file.read()\n",
    "upper_sentences = upper_sentences.split('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "representatives_upper = set([representative.upper() for representative in list(representatives)])\n",
    "repr_sentences = defaultdict(list)\n",
    "\n",
    "for sentence in upper_sentences:\n",
    "    sentence_words = set(sentence.split(' '))\n",
    "    for word in list(sentence_words & representatives_upper):\n",
    "        repr_sentences[word].append(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/987 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "051326dbe5e44084828642ec5610ab31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_upper_to_lower.txt', 'w') as out_file:\n",
    "    for representative, sentences in tqdm(repr_sentences.items(), total=len(repr_sentences)):\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split(' ')\n",
    "            for idx, word in enumerate(sentence_words):\n",
    "                if word == representative:\n",
    "                    sentence_words[idx] = word.lower()\n",
    "            out_file.write(' '.join(sentence_words) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "with open('task3_corpus.txt', 'w') as out_file:\n",
    "    for file in ['task3_polish_lower_clean.txt', 'task3_polish_upper_clean.txt', 'task3_lower_to_upper.txt', 'task3_upper_to_lower.txt']:\n",
    "        with open(file, 'r') as in_file:\n",
    "            shutil.copyfileobj(in_file, out_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task3_corpus.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "model.save('task3.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "upper_words = list(pd.Series(upper_sentences).apply(lambda sentence: str(sentence).split(' ')).explode().unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "upper_words = np.array([word for word in upper_words if len(word) > 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "representatives = list(representatives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def evaluate(word: str, model: Word2Vec):\n",
    "    distances = model.wv.distances(word, other_words=list(upper_words))\n",
    "    try:\n",
    "        position = np.argwhere(upper_words[np.argsort(distances)] == word.upper())[0][0]\n",
    "    except:\n",
    "        print(word)\n",
    "        return 0\n",
    "    return 1 / (position+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def most_similar(word: str, model: Word2Vec, n: int = 10):\n",
    "    distances = model.wv.distances(word, other_words=list(upper_words))\n",
    "    return upper_words[np.argsort(distances)][:n]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/987 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "438cfb69c8dc4ec485edd1e919fae626"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8853681755643011\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for word in tqdm(representatives):\n",
    "    scores.append(evaluate(word, model))\n",
    "\n",
    "print(np.mean(scores))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "scores_pdf = pd.DataFrame(np.array([representatives, scores]).T, columns=['word', 'score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "         word                   score\n707     czyli  0.00010447137484329294\n60      który    0.000117096018735363\n96        już  0.00012584948401711554\n317       być  0.00021777003484320557\n677   również  0.00037243947858472997\n..        ...                     ...\n353  zmieniać                     1.0\n672         i   3.792188092529389e-05\n195        on   3.934374631152378e-05\n400       ten   3.994088748651995e-05\n902         w   4.368147468658542e-05\n\n[987 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>707</th>\n      <td>czyli</td>\n      <td>0.00010447137484329294</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>który</td>\n      <td>0.000117096018735363</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>już</td>\n      <td>0.00012584948401711554</td>\n    </tr>\n    <tr>\n      <th>317</th>\n      <td>być</td>\n      <td>0.00021777003484320557</td>\n    </tr>\n    <tr>\n      <th>677</th>\n      <td>również</td>\n      <td>0.00037243947858472997</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>zmieniać</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>672</th>\n      <td>i</td>\n      <td>3.792188092529389e-05</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>on</td>\n      <td>3.934374631152378e-05</td>\n    </tr>\n    <tr>\n      <th>400</th>\n      <td>ten</td>\n      <td>3.994088748651995e-05</td>\n    </tr>\n    <tr>\n      <th>902</th>\n      <td>w</td>\n      <td>4.368147468658542e-05</td>\n    </tr>\n  </tbody>\n</table>\n<p>987 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_pdf.sort_values('score')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['FTP', 'POCIĄGNIĘTY', 'IMO', 'ZUPELNIE', 'SIMPLE', 'WIDE',\n       'NORYMBERSKI', 'PU', 'ARKANSAS', 'ZASZCZEPIONY'], dtype='<U21')"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('czyli', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['LIPIEC', 'PAŹDZIERNIK', 'CZERWIEC', 'MARZEC', 'KWIECIEŃ',\n       'WRZESIEŃ', 'MAJ', 'GRUDZIEŃ', 'STYCZEŃ', 'SIERPIEŃ'], dtype='<U21')"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('czerwiec', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['SŁOWACKĄ', 'BIALSKI', 'TRAVEL', 'KABUL', 'IRINA', 'MIEDZIOWY',\n       'LESS', 'OTWOCKI', 'GDYZ', 'DOPÓTY'], dtype='<U21')"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('ustawa', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "lower_words = lower_word_counts.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "lower_words = np.array([word for word in lower_words if len(word) > 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35a295fb66c14628bab5a0cbe506e40d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tank\n",
      "being\n",
      "around\n",
      "tkacz\n",
      "orawa\n",
      "pablo\n",
      "without\n",
      "material\n",
      "ą\n",
      "never\n",
      "dy\n",
      "vel\n",
      "wnanie\n",
      "organization\n",
      "survey\n",
      "creative\n",
      "effect\n",
      "ever\n",
      "hair\n",
      "widzialem\n",
      "help\n",
      "0.11168356617492191\n"
     ]
    }
   ],
   "source": [
    "random_words = np.random.choice(lower_words, 5000, replace=False)\n",
    "scores = []\n",
    "for word in tqdm(random_words):\n",
    "    scores.append(evaluate(word, model))\n",
    "\n",
    "print(np.mean(scores))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "scores_pdf = pd.DataFrame(np.array([random_words, scores]).T, columns=['word', 'score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0                       400\n0.5                       106\n0.3333333333333333         70\n0.25                       53\n0.2                        42\n                         ... \n0.00020161290322580645      1\n0.00017844396859386153      1\n0.00011280315848843768      1\n0.0002907822041291073       1\n8.864462370357238e-05       1\nName: score, Length: 2450, dtype: int64"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_pdf['score'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['SŁUŻEBNY', 'RODNIK', 'WÓDKA', 'JEDNOLIŚCIENNY', 'SŁOWIAN',\n       'ABDULLAH', 'WOŁOWY', 'POGŁOWIE', 'BOROWIK', 'BIUROKRACJA'],\n      dtype='<U21')"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('twaróg', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['TWARÓG', 'PIECHA', 'KALETA', 'KAMIŃSKA', 'RAFALSKA', 'URBANIAK',\n       'MARIA', 'ROJEK', 'KRUK', 'JACEK'], dtype='<U21')"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('TWARÓG', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['KWADRANS', 'CAL', 'SOBOTA', 'TYDZIEŃ', 'WYEMITOWAĆ', 'WPŁACIĆ',\n       'MIESIĄC', 'SWIFT', 'MINĄĆ', 'SLIM'], dtype='<U21')"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('KWADRANS', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "               word                  score\n2475         wnanie                      0\n1716       material                      0\n1944              ą                      0\n1343          orawa                      0\n655           being                      0\n...             ...                    ...\n3107        brylant  9.956192751891676e-05\n1296     odpowiedni  9.961151509114453e-05\n732   wyprzedzający  9.974067424695791e-05\n4909      katechizm  9.986019572598362e-05\n1957   niesamowicie  9.987016878058524e-05\n\n[5000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2475</th>\n      <td>wnanie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1716</th>\n      <td>material</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1944</th>\n      <td>ą</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1343</th>\n      <td>orawa</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>655</th>\n      <td>being</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3107</th>\n      <td>brylant</td>\n      <td>9.956192751891676e-05</td>\n    </tr>\n    <tr>\n      <th>1296</th>\n      <td>odpowiedni</td>\n      <td>9.961151509114453e-05</td>\n    </tr>\n    <tr>\n      <th>732</th>\n      <td>wyprzedzający</td>\n      <td>9.974067424695791e-05</td>\n    </tr>\n    <tr>\n      <th>4909</th>\n      <td>katechizm</td>\n      <td>9.986019572598362e-05</td>\n    </tr>\n    <tr>\n      <th>1957</th>\n      <td>niesamowicie</td>\n      <td>9.987016878058524e-05</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_pdf.sort_values('score')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "random_words = np.random.choice(lower_word_counts.index, 5000, p=lower_word_counts.values/np.sum(lower_word_counts), replace=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01991764f83f434f93cd6a89ae3455ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural\n",
      "0.26995765952335005\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for word in tqdm(random_words):\n",
    "    scores.append(evaluate(word, model))\n",
    "\n",
    "print(np.mean(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "scores_pdf = pd.DataFrame(np.array([random_words, scores]).T, columns=['word', 'score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "1.000000    1092\n0.500000     184\n0.333333     141\n0.250000     107\n0.200000      75\n            ... \n0.000309       1\n0.000049       1\n0.000345       1\n0.000426       1\n0.003704       1\nName: score, Length: 1580, dtype: int64"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_pdf['score'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "            word     score\n3135    cultural         0\n113        tylko  0.000038\n60             i  0.000038\n125       jednak  0.000038\n392      właśnie  0.000038\n...          ...       ...\n4158  wydłużenie       1.0\n4157     wykonać       1.0\n1806  decydujący       1.0\n1831     składać       1.0\n4999     ciekawy       1.0\n\n[5000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3135</th>\n      <td>cultural</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>tylko</td>\n      <td>0.000038</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>i</td>\n      <td>0.000038</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>jednak</td>\n      <td>0.000038</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>właśnie</td>\n      <td>0.000038</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4158</th>\n      <td>wydłużenie</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4157</th>\n      <td>wykonać</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1806</th>\n      <td>decydujący</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1831</th>\n      <td>składać</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>ciekawy</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_pdf.sort_values('score')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "1665"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_word_counts['rzeczywiście']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['CLOUD', 'FINALNIE', 'LC', 'SEARCH', 'KTOREJ', 'SKOWROŃSKĄ',\n       'ŚWIDNIKU', 'PŁUCNY', 'UNIEWINNIONY', 'UBYĆ'], dtype='<U21')"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('rzeczywiście', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['DUALIZM', 'POCHOPNY', 'MOBBING', 'KRLD', 'OBLIGATORYJNIE',\n       'NADLEŚNICTWO', 'KATEGORYCZNY', 'SPRAWIEDLIWIE', 'POKORNY',\n       'TOLEROWAĆ'], dtype='<U21')"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('piekarnia', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "4947e307",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 4 (4 points)\n",
    "\n",
    "In this task you are asked to do two things:\n",
    "1. compare the embeddings computed on small corpus (like Brown Corpus , see: <https://en.wikipedia.org/wiki/Brown_Corpus>) with the ones coming from Google News Corpus\n",
    "2. Try to use other resourses like WordNet to enrich to corpus, and obtain better embeddings\n",
    "\n",
    "You can use the following code snippets:\n",
    "\n",
    "```python\n",
    "# printing tokenized Brown Corpora\n",
    "from nltk.corpus import brown\n",
    "for s in brown.sents():\n",
    "    print(*s)\n",
    "    \n",
    "#iterating over all synsets in WordNet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
    "    for synset in list(wn.all_synsets(synset_type)))[:10]:\n",
    "        print (synset.definition())\n",
    "        print (synset.examples())\n",
    "        print ([lem.name() for lem in synset.lemmas()])\n",
    "        print (synset.hyperonims()) # nodes 1 level up in ontology\n",
    "        \n",
    "# loading model and compute cosine similarity between words\n",
    "\n",
    "model = Word2Vec.load('models/w2v.wordnet5.model') \n",
    "print (model.wv.similarity('dog', 'cat'))\n",
    "```\n",
    "\n",
    "Embeddings will be tested using WordSim-353 dataset, the code showing the quality is in the cell below. Prepare the following corpora:\n",
    "1. Tokenized Brown Corpora\n",
    "2. Definitions and examples from Princeton WordNet\n",
    "3. (1) and (2) together\n",
    "4. (3) enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')\n",
    "\n",
    "Train 4 Word2Vec models, and raport Spearman correletion between similarities based on your vectors, and similarities based on human judgements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a2fbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code for computing correlation between W2V similarity, and human judgements\n",
    "\n",
    "import gensim.downloader\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "gn = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "for similarity_type in ['relatedness', 'similarity']:\n",
    "    ws353 = []\n",
    "    for x in open(f'wordsim_{similarity_type}_goldstandard.txt'): \n",
    "        a,b,val = x.split()\n",
    "        val = float(val)\n",
    "        ws353.append( (a,b,val))\n",
    "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
    "    print (similarity_type + ':', spearmanr(vals, ys)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf71c95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}