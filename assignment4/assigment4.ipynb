{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef34449",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assigment 4\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* get at least 4 points by Tuesday, 12.05.2022\n",
    "* remaining points: last lab session before or on Tuesday, 19.05.2022\n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1HaMbhzaBxxNa_z_QJXSDCbv5VddmhVVZ?usp=sharing> (or will be soon :) )\n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss from Slide 3 in Lecture NLP.2, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "data_pdf = pd.read_csv('task1_objects_contexts_polish.txt', sep=' ', names=['object', 'context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "object_encoder = LabelEncoder()\n",
    "data_pdf['object'] = object_encoder.fit_transform(data_pdf['object'])\n",
    "\n",
    "context_encoder = LabelEncoder()\n",
    "data_pdf['context'] = context_encoder.fit_transform(data_pdf['context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, data_pdf):\n",
    "        self.positive_samples = data_pdf.groupby('context').apply(lambda pdf: set(pdf['object'].unique()))\n",
    "        self.object_distribution = np.power(data_pdf['object'].value_counts(), 3/4)\n",
    "        self.all_samples = set(data_pdf['object'].unique())\n",
    "\n",
    "    def sample(self, context, k):\n",
    "        negative_samples = list(self.all_samples - self.positive_samples[context])\n",
    "        negative_distribution = self.object_distribution[negative_samples]\n",
    "        negative_distribution /= np.sum(negative_distribution)\n",
    "        return np.random.choice(negative_distribution.index, size=k, replace=False, p=negative_distribution.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def object_grad(object_embedding, context_embedding):\n",
    "    return -(1-sigmoid(np.dot(object_embedding, context_embedding)))*context_embedding\n",
    "\n",
    "def context_grad(object_embedding, context_embedding, negative_embeddings):\n",
    "    return (-(1-sigmoid(object_embedding@context_embedding))*object_embedding\n",
    "            + np.sum((1-sigmoid(-context_embedding.reshape(1, -1) @ negative_embeddings.T))*negative_embeddings.T, axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def loss_fun(object_embedding, context_embedding, negative_embeddings):\n",
    "    return (-np.log(sigmoid(object_embedding @ context_embedding))\n",
    "            - np.sum(sigmoid(-context_embedding.reshape(1, -1) @ negative_embeddings.T)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, lr: float, k: int, embeddings_size: int):\n",
    "        self.lr = lr\n",
    "        self.k = k\n",
    "        self.object_embeddings = np.random.rand(len(data_pdf['object'].unique()), embeddings_size)\n",
    "        self.context_embeddings = np.random.rand(len(data_pdf['context'].unique()), embeddings_size)\n",
    "\n",
    "    def step(self, object, context, samples):\n",
    "        negative_objects = np.array(samples)\n",
    "        self.object_embeddings[object] -= self.lr*object_grad(self.object_embeddings[object], self.context_embeddings[context])\n",
    "        self.context_embeddings[context] -= self.lr*context_grad(self.object_embeddings[object], self.context_embeddings[context], self.object_embeddings[negative_objects])\n",
    "        return loss_fun(self.object_embeddings[object], self.context_embeddings[context], self.object_embeddings[negative_objects])\n",
    "\n",
    "    def run_epoch(self, data, log_freq, distribution):\n",
    "        negative_samples = np.random.choice(distribution.index, p=distribution.values, size=(len(data), self.k))\n",
    "        iter = 0\n",
    "        loss = 0\n",
    "        for object, context in tqdm(data):\n",
    "            loss += self.step(object, context, negative_samples[iter])\n",
    "            iter += 1\n",
    "            if log_freq is not None and iter % log_freq == 0:\n",
    "                print('loss: ', loss/iter)\n",
    "\n",
    "    def fit(self, data_pdf, num_epochs, log_freq = 1000000):\n",
    "        distribution = np.power(data_pdf['object'].value_counts(), 3/4)\n",
    "        distribution /= sum(distribution)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(epoch)\n",
    "            self.run_epoch(data_pdf.values, log_freq, distribution)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c4cda658b0b4f93b93d2dc56d3746b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -0.976483554272937\n",
      "loss:  -1.3400992543096206\n",
      "loss:  -1.5321450409319193\n",
      "loss:  -1.658324154624081\n",
      "loss:  -1.7505253612235852\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c633381c25b4a76a63c3e7641e50dd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.24171407203101\n",
      "loss:  -2.270901648276874\n",
      "loss:  -2.2904629613822967\n",
      "loss:  -2.3048811849044286\n",
      "loss:  -2.315929483039553\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4388847fc9464c81ba1b35ff738f2940"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3927689334846476\n",
      "loss:  -2.4005408104554\n",
      "loss:  -2.403134796950425\n",
      "loss:  -2.4043327410947035\n",
      "loss:  -2.404318121539505\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53f723d35e46461698897aa8157f9a59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.4091351594575796\n",
      "loss:  -2.4091184871845384\n",
      "loss:  -2.40660682144957\n",
      "loss:  -2.403637415284451\n",
      "loss:  -2.4004625914182163\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ca36571f20545abbf4a458087207360"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.38529273200978\n",
      "loss:  -2.384298887248675\n",
      "loss:  -2.3816417914161003\n",
      "loss:  -2.379505673691447\n",
      "loss:  -2.376775936987627\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b0779ddf5914644be240c4d2494b7c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.365745142543159\n",
      "loss:  -2.365264209055516\n",
      "loss:  -2.362773496095009\n",
      "loss:  -2.3605348128264025\n",
      "loss:  -2.3581710022408773\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d7770a5c7284abeb48a81c8a5f5a962"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.351082214021668\n",
      "loss:  -2.350176746681713\n",
      "loss:  -2.3482948966123516\n",
      "loss:  -2.347176325622098\n",
      "loss:  -2.3455854880115394\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2c2c6df275e4d6e845a65958f7f4904"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.341554906687945\n",
      "loss:  -2.341195241428283\n",
      "loss:  -2.3401067604251096\n",
      "loss:  -2.339340450009273\n",
      "loss:  -2.3385643535662677\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd5c561ea5a8487b85916db67619f85d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3378953289502684\n",
      "loss:  -2.337793450528365\n",
      "loss:  -2.337329117158638\n",
      "loss:  -2.3373217386999716\n",
      "loss:  -2.3368739804082472\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66c630a2a4514fd0964245e675dbb3ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3382983613315256\n",
      "loss:  -2.3388867824362958\n",
      "loss:  -2.3384812736516216\n",
      "loss:  -2.3383699275041407\n",
      "loss:  -2.338142029682852\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a8e99c851834d1287319f657065e827"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.342033898324023\n",
      "loss:  -2.3426512288061803\n",
      "loss:  -2.342438706678854\n",
      "loss:  -2.342877583124321\n",
      "loss:  -2.3430062163415735\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "561691a4daf74251a10b854c6c1e0723"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3476531092380117\n",
      "loss:  -2.3484678671646053\n",
      "loss:  -2.3486585503966606\n",
      "loss:  -2.349268881553964\n",
      "loss:  -2.3496627214495716\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22b626c523474b19b24ff6672c8413cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3566118121149886\n",
      "loss:  -2.3572747522531126\n",
      "loss:  -2.3576562897153446\n",
      "loss:  -2.3580801144250216\n",
      "loss:  -2.358673227115826\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f81923cd5464d40aa99d4e97f560b78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.365580979058521\n",
      "loss:  -2.3662356200597117\n",
      "loss:  -2.367095737430774\n",
      "loss:  -2.3678132062664488\n",
      "loss:  -2.36843804281139\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5b532f666744b9d8fc5b7995e92aa98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.376241827862562\n",
      "loss:  -2.377153089262103\n",
      "loss:  -2.377807766103135\n",
      "loss:  -2.3786442734349085\n",
      "loss:  -2.379277821616192\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169c231c16a64523ba23aec1c455f401"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3863342654820494\n",
      "loss:  -2.387491426237623\n",
      "loss:  -2.388458916139939\n",
      "loss:  -2.3894602838052874\n",
      "loss:  -2.390311416760087\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b00378c25f1e4d4e80831d72cf3e68bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.3975970855434103\n",
      "loss:  -2.398661025792496\n",
      "loss:  -2.3995538291099727\n",
      "loss:  -2.400741819020991\n",
      "loss:  -2.4016317781935426\n",
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e748f36b59154218bfc2d876655cb2ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.410046844944847\n",
      "loss:  -2.4109064001713154\n",
      "loss:  -2.4117589725494937\n",
      "loss:  -2.412471475428429\n",
      "loss:  -2.413321914258733\n",
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b86fe66a3d294641a40f0ee654f1eb82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.4214263697881115\n",
      "loss:  -2.421966135179359\n",
      "loss:  -2.4233579476737837\n",
      "loss:  -2.424311271507467\n",
      "loss:  -2.425219977958236\n",
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5525116 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e142b19065574bc1a01633452985df31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  -2.432611052232172\n",
      "loss:  -2.433632762885281\n",
      "loss:  -2.4350643908244107\n",
      "loss:  -2.4361109515654737\n",
      "loss:  -2.437166534018982\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(0.01, 5, 100)\n",
    "word2vec.fit(data_pdf, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "embeddings = word2vec.object_embeddings\n",
    "embeddings_pdf = pd.DataFrame(embeddings)\n",
    "embeddings_pdf.index = object_encoder.inverse_transform(embeddings_pdf.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "with open('task1_w2v_vectors.txt', 'w') as out_file:\n",
    "    out_file.write(f'{len(embeddings_pdf)} {len(embeddings_pdf.columns)}\\n')\n",
    "    embeddings_pdf.to_csv(out_file, header=False, index=True, sep=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ae909d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    kot 0.8010485768318176\n",
      "    koń 0.7975372076034546\n",
      "    zwierzę 0.7651735544204712\n",
      "    chłopiec 0.7597785592079163\n",
      "    dziewczyna 0.7183960676193237\n",
      "    mężczyzna 0.7060914635658264\n",
      "    chłopak 0.700786828994751\n",
      "    dziewczynka 0.6936959028244019\n",
      "    ptak 0.6886632442474365\n",
      "    kobieta 0.6841621398925781\n",
      "\n",
      "WORD: smok\n",
      "    niedźwiedź 0.6620179414749146\n",
      "    bocian 0.6596109867095947\n",
      "    bestia 0.6152178049087524\n",
      "    dzieciak 0.6127355098724365\n",
      "    ptak 0.6104136109352112\n",
      "    potwór 0.6066800355911255\n",
      "    tygrys 0.6056229472160339\n",
      "    słoń 0.6001629829406738\n",
      "    wilk 0.5967732667922974\n",
      "    wąż 0.5922409296035767\n",
      "\n",
      "WORD: miłość\n",
      "    wiara 0.7513876557350159\n",
      "    duch 0.6542425751686096\n",
      "    uczucie 0.6329718232154846\n",
      "    śmierć 0.627207338809967\n",
      "    natura 0.6237786412239075\n",
      "    młodość 0.6211560964584351\n",
      "    wolność 0.6175273656845093\n",
      "    radość 0.6165459752082825\n",
      "    miłosierdzie 0.6136339902877808\n",
      "    wyobraźnia 0.6124131083488464\n",
      "\n",
      "WORD: rower\n",
      "    auto 0.6595624089241028\n",
      "    wózek 0.6472136378288269\n",
      "    ciężarówka 0.6223849058151245\n",
      "    torba 0.6204795241355896\n",
      "    plecak 0.6048317551612854\n",
      "    motocykl 0.6007189154624939\n",
      "    rakieta 0.5908415913581848\n",
      "    pociąg 0.5879620313644409\n",
      "    paczka 0.5861489772796631\n",
      "    autobus 0.5833216905593872\n",
      "\n",
      "WORD: maraton\n",
      "    rozgrywka 0.5802261829376221\n",
      "    piknik 0.5518872737884521\n",
      "    rajd 0.5512055158615112\n",
      "    wyścig 0.5315948724746704\n",
      "    sesja 0.5302449464797974\n",
      "    zmaganie 0.5217149257659912\n",
      "    turniej 0.5203548669815063\n",
      "    mecz 0.5055239200592041\n",
      "    finał 0.4991457164287567\n",
      "    potyczka 0.4986191391944885\n",
      "\n",
      "WORD: logika\n",
      "    swoboda 0.6528512835502625\n",
      "    prostota 0.6490967869758606\n",
      "    spójność 0.6462158560752869\n",
      "    złożoność 0.6453420519828796\n",
      "    efektywność 0.6338891386985779\n",
      "    specyfika 0.6332631707191467\n",
      "    estetyka 0.630141019821167\n",
      "    uczciwość 0.6301333904266357\n",
      "    powaga 0.6250840425491333\n",
      "    motywacja 0.6248546838760376\n",
      "\n",
      "WORD: motyl\n",
      "    ptak 0.6189852356910706\n",
      "    świnia 0.5453046560287476\n",
      "    potwór 0.544842004776001\n",
      "    smok 0.5391629338264465\n",
      "    pszczoła 0.5320708751678467\n",
      "    owad 0.5124348402023315\n",
      "    jajo 0.5077357292175293\n",
      "    szczur 0.5058262348175049\n",
      "    bomba 0.5008795261383057\n",
      "    stop 0.499335378408432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors.txt', binary=False)\n",
    "\n",
    "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
    "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "example_words = example_polish_words\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task1_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2 (4 points)\n",
    "\n",
    "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
    "\n",
    "```python\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "```\n",
    "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file] containing such pairs of titles, that one article links to another.\n",
    "\n",
    "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
    "\n",
    "Compare these two approaches using similar code to the code from Task 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6ad8e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wiki_links_pdf = pd.read_csv('task2_simple.wiki.links.txt', sep=' ', names=['object', 'context'])\n",
    "# The cell for your presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "wiki_titles = set(np.hstack((wiki_links_pdf['object'], wiki_links_pdf['context'])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "word_counts = pd.Series(list(wiki_titles)).apply(lambda title: str(title).split('_')).explode().value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "rare_words = word_counts[(word_counts > 1) & (word_counts <= 30)].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79ac7e430df242c588ae4f18a1a2c4f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles = defaultdict(list)\n",
    "wiki_titles = list(wiki_titles)\n",
    "rare_words = set(rare_words)\n",
    "\n",
    "for it, title in tqdm(enumerate(wiki_titles)):\n",
    "    for word in list(set(str(title).split('_')) & rare_words):\n",
    "        articles[word].append(it)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/204926 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb0c898d45b840e49fa50c51d5c27b29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task2_related_wiki_links.txt', 'w') as out_file:\n",
    "    for vals in tqdm(articles.values()):\n",
    "        for pair in combinations(vals, 2):\n",
    "            out_file.write(f'{wiki_titles[pair[0]]} {wiki_titles[pair[1]]}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "example_titles = [\n",
    "    'capital_city',\n",
    "    'flower',\n",
    "    'mickey_mouse',\n",
    "    'finance',\n",
    "    'japan_national_under-23_football_team',\n",
    "    'john_the_apostle',\n",
    "    'boss_(gaming)',\n",
    "    'the_wealth_of_nations',\n",
    "    'sulfur_dioxide',\n",
    "    'tandem_bicycle',\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_simple.wiki.links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: capital_city\n",
      "    population_density 0.739251434803009\n",
      "    province 0.7069327235221863\n",
      "    plain 0.701899528503418\n",
      "    oceanic_climate 0.6916329264640808\n",
      "    köppen_climate_classification 0.6897584199905396\n",
      "    list_of_capital_cities_by_altitude 0.6814950704574585\n",
      "    category:departments_of_paraguay 0.6782993674278259\n",
      "    above_sea_level 0.6735855340957642\n",
      "    category:departments_of_the_republic_of_the_congo 0.6711230874061584\n",
      "    hill 0.669350266456604\n",
      "\n",
      "WORD: flower\n",
      "    asteraceae 0.8791819214820862\n",
      "    leaf 0.8778598308563232\n",
      "    berry 0.8747152090072632\n",
      "    vine 0.8712254166603088\n",
      "    seed 0.8700944781303406\n",
      "    perennial 0.8616876602172852\n",
      "    tail 0.8591261506080627\n",
      "    fruit 0.856979489326477\n",
      "    sweet_(taste) 0.8559210300445557\n",
      "    lime 0.8551871180534363\n",
      "\n",
      "WORD: mickey_mouse\n",
      "    minnie_mouse 0.9010185599327087\n",
      "    goofy 0.8845751285552979\n",
      "    bugs_bunny 0.8688761591911316\n",
      "    donald_duck 0.8686318397521973\n",
      "    daffy_duck 0.857617974281311\n",
      "    pluto_(disney) 0.8547376990318298\n",
      "    walt_disney_company 0.8538717031478882\n",
      "    tokyo_disneyland 0.8529824614524841\n",
      "    daisy_duck 0.8525812029838562\n",
      "    fred_moore_(animator) 0.8519846796989441\n",
      "\n",
      "WORD: finance\n",
      "    investment 0.859107494354248\n",
      "    banking 0.8380959630012512\n",
      "    insurance 0.828545868396759\n",
      "    stock_market 0.8262161016464233\n",
      "    financial 0.819246232509613\n",
      "    auction 0.8171119689941406\n",
      "    shareholder 0.8169492483139038\n",
      "    business 0.8091278076171875\n",
      "    share_(finance) 0.8073405623435974\n",
      "    wikt:found 0.8066060543060303\n",
      "\n",
      "WORD: japan_national_under-23_football_team\n",
      "    hong_kong_national_football_team 0.9604832530021667\n",
      "    2011_afc_asian_cup 0.9599856734275818\n",
      "    football_at_the_1966_asian_games 0.9528710246086121\n",
      "    2019_afc_asian_cup 0.951862633228302\n",
      "    football_at_the_1970_asian_games 0.9510948061943054\n",
      "    1988_afc_asian_cup 0.9494122862815857\n",
      "    football_at_the_1986_asian_games 0.9492978453636169\n",
      "    2000_afc_asian_cup 0.948467493057251\n",
      "    football_at_the_2002_asian_games_–_men's_tournament 0.9473084211349487\n",
      "    football_at_the_1978_asian_games 0.9464484453201294\n",
      "\n",
      "WORD: john_the_apostle\n",
      "    gospel_of_mark 0.978923499584198\n",
      "    acts_of_the_apostles 0.9740555286407471\n",
      "    matthew_the_evangelist 0.9712485671043396\n",
      "    jerome 0.9652677774429321\n",
      "    category:ancient_christianity 0.9603877663612366\n",
      "    theotokos 0.960051417350769\n",
      "    nicene_creed 0.9586400389671326\n",
      "    gospel_of_luke 0.9579897522926331\n",
      "    origen 0.9572450518608093\n",
      "    paul_of_tarsus 0.9551813006401062\n",
      "\n",
      "WORD: boss_(gaming)\n",
      "    nascar_07 0.9553720951080322\n",
      "    list_of_acclaim_entertainment_subsidiaries#acclaim_studios_cheltenham 0.9435751438140869\n",
      "    onlive 0.9433116912841797\n",
      "    british_academy_games_awards 0.942517876625061\n",
      "    megatech 0.9415945410728455\n",
      "    conficker_a 0.9402057528495789\n",
      "    barney_calhoun 0.9397355914115906\n",
      "    cs_go 0.9394882321357727\n",
      "    barbie_horse_adventures_ii:_riding_camp 0.9381359815597534\n",
      "    14_degrees_east 0.937997579574585\n",
      "\n",
      "WORD: the_wealth_of_nations\n",
      "    ricardian_model 0.952191174030304\n",
      "    neoclassical_economics 0.9482550621032715\n",
      "    the_theory_of_moral_sentiments 0.9475444555282593\n",
      "    classical_economics 0.9471796751022339\n",
      "    category:anarchism 0.943265438079834\n",
      "    economies_of_scale 0.9426497220993042\n",
      "    public_good 0.9414759874343872\n",
      "    category:economic_theories 0.9397423267364502\n",
      "    category:economic_systems 0.9373308420181274\n",
      "    open_market 0.9350383281707764\n",
      "\n",
      "WORD: sulfur_dioxide\n",
      "    crystallization 0.9755793213844299\n",
      "    sulfuric_acid 0.9725997447967529\n",
      "    magnetite 0.9715259075164795\n",
      "    specific_gravity 0.9689416289329529\n",
      "    malleable 0.967831552028656\n",
      "    sodium_chloride 0.9677532911300659\n",
      "    titanium_dioxide 0.967487633228302\n",
      "    uranyl_nitrate 0.9673746824264526\n",
      "    carbonic_acid 0.966763436794281\n",
      "    carbon_steel 0.9667571187019348\n",
      "\n",
      "WORD: tandem_bicycle\n",
      "    elena_marsili 0.9514120221138\n",
      "    crown_prince_of_greece 0.95046466588974\n",
      "    category:tunisian_producers 0.9477298259735107\n",
      "    jerzy_kowalczyk 0.9473161101341248\n",
      "    category:belgian_atheists 0.9470301866531372\n",
      "    josé_agustín_goytisolo 0.9466261267662048\n",
      "    category:guyanese_writers 0.9465793371200562\n",
      "    swimming_at_the_1992_summer_paralympics 0.9458359479904175\n",
      "    premio_alfaguara 0.9450024962425232\n",
      "    mingo_y_aníbal_contra_los_fantasmas 0.9446794986724854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_simple_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task2_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_related_wiki_links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: capital_city\n",
      "capital_city not found in the training set\n",
      "\n",
      "WORD: flower\n",
      "flower not found in the training set\n",
      "\n",
      "WORD: mickey_mouse\n",
      "mickey_mouse not found in the training set\n",
      "\n",
      "WORD: finance\n",
      "finance not found in the training set\n",
      "\n",
      "WORD: japan_national_under-23_football_team\n",
      "japan_national_under-23_football_team not found in the training set\n",
      "\n",
      "WORD: john_the_apostle\n",
      "john_the_apostle not found in the training set\n",
      "\n",
      "WORD: boss_(gaming)\n",
      "    first-person_(gaming) 0.974440336227417\n",
      "    item_(gaming) 0.97352534532547\n",
      "    match_(gaming) 0.9721232056617737\n",
      "    ken_williams_(gaming) 0.971487283706665\n",
      "    deathmatch_(gaming) 0.9703531861305237\n",
      "    spam_(gaming) 0.966529130935669\n",
      "    kana_kitahara 0.9655419588088989\n",
      "    stalling_(gaming) 0.9654951691627502\n",
      "    first_person_(gaming) 0.9649636149406433\n",
      "    toa_alta,_puerto_rico 0.9643204212188721\n",
      "\n",
      "WORD: the_wealth_of_nations\n",
      "    wealth_management 0.9905750751495361\n",
      "    morgan_stanley_wealth_management 0.9884107112884521\n",
      "    list_of_countries_by_distribution_of_wealth 0.9877914190292358\n",
      "    appeal_to_wealth 0.9875590801239014\n",
      "    wealth_(movie) 0.9873790144920349\n",
      "    gross_domestic_product#standard_of_living_and_gdp:_wealth_distribution_and_externalities 0.9867001175880432\n",
      "    sovereign_wealth_fund 0.9865601658821106\n",
      "    evercore_wealth_management 0.9864317178726196\n",
      "    wealth_gap_in_the_united_states 0.986093282699585\n",
      "    wealth_redistribution 0.9860005378723145\n",
      "\n",
      "WORD: sulfur_dioxide\n",
      "    image:mauna_loa_sulfur_hexafluoride_concentration.jpg 0.9789128303527832\n",
      "    sulfur_trioxide 0.9754117131233215\n",
      "    rhenium_dioxide 0.974718451499939\n",
      "    bromine_dioxide 0.9737715125083923\n",
      "    sulfur_pentafluoride 0.973708987236023\n",
      "    sulfur_monoxide 0.9730807542800903\n",
      "    selenium_dioxide 0.9724143743515015\n",
      "    sulfur_dichloride 0.9720863103866577\n",
      "    sulfur_nitride 0.9719774127006531\n",
      "    sulfur_tetrachloride 0.97173672914505\n",
      "\n",
      "WORD: tandem_bicycle\n",
      "    amino_acids 0.9693626165390015\n",
      "    argonne_tandem_linear_accelerator_system 0.9685113430023193\n",
      "    tandem_repeat 0.9683820605278015\n",
      "    aren't_you_glad_you're_you? 0.9681868553161621\n",
      "    akari_inaba 0.9663731455802917\n",
      "    retro_vhs_blu-ray 0.9661436676979065\n",
      "    recommended_citation:_frost,_darrel_r._2019._amphibian_species_of_the_world:_an_online_reference._version_6.0_(date_of_access)._electronic_database_accessible_at_http://research.amnh.org/herpetology/amphibia/index.html._american_museum_of_natural_history,_new_york,_usa. 0.9659613966941833\n",
      "    file:us_navy_070412-n-7498l-018_a_french_super-etendard_from_the_nuclear-powered_aircraft_carrier_french_navy_ship_charles_de_gaulle_(r_91)_performs_a_touch-and-go_landing_on_the_flight_deck_of_the_nimitz-class_aircraft_carrier_uss.jpg 0.965947687625885\n",
      "    bitlis_vilayet 0.965805172920227\n",
      "    drying_rack 0.9657403826713562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_related_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    try:\n",
    "        for w, v in task2_wv.most_similar(w):\n",
    "            print ('   ', w, v)\n",
    "    except:\n",
    "        print(f'{w} not found in the training set')\n",
    "    print ()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import shutil\n",
    "with open('task2_combined_wiki_links.txt', 'wb') as out_file:\n",
    "    for file in ['task2_related_wiki_links.txt', 'task2_simple.wiki.links.txt' ]:\n",
    "        with open(file, 'rb') as in_file:\n",
    "            shutil.copyfileobj(in_file, out_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task2_combined_wiki_links.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: capital_city\n",
      "    list_of_national_capitals 0.7547511458396912\n",
      "    population_density 0.7497968077659607\n",
      "    list_of_capital_cities_by_altitude 0.745806872844696\n",
      "    province 0.727108359336853\n",
      "    köppen_climate_classification 0.7214742302894592\n",
      "    communes_of_burundi 0.7195709943771362\n",
      "    port 0.712222158908844\n",
      "    hill 0.7095301151275635\n",
      "    category:regions_of_burkina_faso 0.7092888355255127\n",
      "    coast 0.7067225575447083\n",
      "\n",
      "WORD: flower\n",
      "    berry 0.9138245582580566\n",
      "    evergreen 0.91241455078125\n",
      "    asteraceae 0.9113523364067078\n",
      "    leaf 0.9072189927101135\n",
      "    flowers 0.8990909457206726\n",
      "    sweet_(taste) 0.8957048654556274\n",
      "    buckwheat 0.8947560787200928\n",
      "    fruit 0.8932700157165527\n",
      "    orchidaceae 0.8918755054473877\n",
      "    shrub 0.8892049193382263\n",
      "\n",
      "WORD: mickey_mouse\n",
      "    donald_duck 0.9323548078536987\n",
      "    pluto_(disney) 0.9035681486129761\n",
      "    american_pekin_duck 0.8992806673049927\n",
      "    money_bin 0.8984423279762268\n",
      "    goofy 0.8974982500076294\n",
      "    daisy_duck 0.8972910642623901\n",
      "    category:mickey_mouse_universe_characters 0.8919526934623718\n",
      "    huey,_dewey_and_louie 0.8915558457374573\n",
      "    super_ducktales 0.8892890214920044\n",
      "    fred_moore_(animator) 0.8882454037666321\n",
      "\n",
      "WORD: finance\n",
      "    investment 0.879478931427002\n",
      "    insurance 0.8725610375404358\n",
      "    shareholder 0.8627383708953857\n",
      "    infrastructure 0.8478735089302063\n",
      "    share_(finance) 0.8476279377937317\n",
      "    non-profit 0.8457669019699097\n",
      "    category:commerce 0.8376730680465698\n",
      "    business_school 0.8371889591217041\n",
      "    junior_college 0.8369675278663635\n",
      "    business 0.8355017304420471\n",
      "\n",
      "WORD: japan_national_under-23_football_team\n",
      "    hong_kong_national_football_team 0.9595895409584045\n",
      "    2000_afc_asian_cup 0.956436276435852\n",
      "    2011_afc_asian_cup 0.954825222492218\n",
      "    football_at_the_2010_asian_games_–_men's_tournament 0.9512404799461365\n",
      "    football_at_the_2002_asian_games_–_men's_tournament 0.9485815167427063\n",
      "    football_at_the_2018_asian_games_–_men's_tournament 0.947322428226471\n",
      "    football_at_the_1986_asian_games 0.9472708702087402\n",
      "    football_at_the_1966_asian_games 0.9455299973487854\n",
      "    1996_afc_asian_cup 0.9451062083244324\n",
      "    uae_pro_league 0.9449082612991333\n",
      "\n",
      "WORD: john_the_apostle\n",
      "    gospel_of_mark 0.9818589687347412\n",
      "    theotokos 0.9703783988952637\n",
      "    paul_the_apostle 0.9666560888290405\n",
      "    deuterocanonical 0.964203417301178\n",
      "    transfiguration 0.9638434052467346\n",
      "    christology 0.9635299444198608\n",
      "    book_of_odes_(bible) 0.9619091749191284\n",
      "    development_of_the_new_testament_canon 0.9609437584877014\n",
      "    origen 0.960762619972229\n",
      "    one_holy_catholic_and_apostolic_church 0.9605040550231934\n",
      "\n",
      "WORD: boss_(gaming)\n",
      "    item_(gaming) 0.9903486371040344\n",
      "    unlockable_(gaming) 0.9896479249000549\n",
      "    health_(gaming) 0.9893690943717957\n",
      "    quest_(gaming) 0.9889283180236816\n",
      "    life_(gaming) 0.9881860017776489\n",
      "    mod_(gaming) 0.9881646633148193\n",
      "    match_(gaming) 0.9864320755004883\n",
      "    ken_williams_(gaming) 0.9851617813110352\n",
      "    first_person_(gaming) 0.9826708436012268\n",
      "    game_(simulation) 0.9797883033752441\n",
      "\n",
      "WORD: the_wealth_of_nations\n",
      "    category:people_by_wealth_or_status 0.9434105753898621\n",
      "    national_wealth 0.8976672887802124\n",
      "    redistribution_of_income_and_wealth 0.8891507983207703\n",
      "    gross_domestic_product#standard_of_living_and_gdp:_wealth_distribution_and_externalities 0.8879180550575256\n",
      "    net_wealth 0.8850098848342896\n",
      "    wealth_redistribution 0.8792318105697632\n",
      "    objectivism_(ayn_rand) 0.8792073130607605\n",
      "    the_black_book:_imbalance_of_power_and_wealth_in_the_sudan 0.8788139224052429\n",
      "    category:society 0.8767331838607788\n",
      "    powershift:_knowledge,_wealth_and_violence_at_the_edge_of_the_21st_century 0.874108076095581\n",
      "\n",
      "WORD: sulfur_dioxide\n",
      "    oxide 0.9689789414405823\n",
      "    sulfate 0.9675155878067017\n",
      "    sulfuric_acid 0.9640898108482361\n",
      "    crystalline 0.9618420600891113\n",
      "    hydroxide 0.9603775143623352\n",
      "    iron(ii)_sulfate 0.95839923620224\n",
      "    sodium 0.9582377076148987\n",
      "    iron(iii)_oxide 0.9580594897270203\n",
      "    sodium_chloride 0.9577062726020813\n",
      "    template:chemical_formula/atom 0.9575254917144775\n",
      "\n",
      "WORD: tandem_bicycle\n",
      "    cycling_at_the_2014_commonwealth_games_–_men's_tandem_1km_time_trial_b 0.9946358799934387\n",
      "    cycling_at_the_2014_commonwealth_games_–_men's_tandem_sprint_b 0.9927564859390259\n",
      "    tandem 0.9853826761245728\n",
      "    cycling_at_the_1964_summer_olympics_–_men's_tandem 0.9811911582946777\n",
      "    air_command_tandem 0.9800781607627869\n",
      "    independence_speed_tandem 0.9800019860267639\n",
      "    tandem_solar_cell 0.9799981713294983\n",
      "    harry_boot 0.9638161063194275\n",
      "    it:valvola_termoionica#triodo 0.9607505202293396\n",
      "    short_division 0.959282636642456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_wv = model.wv\n",
    "task2_wv.save('task2_combined_wiki_links_w2v_vectors.txt')\n",
    "example_words = example_titles\n",
    "\n",
    "for w in example_words:\n",
    "    print('WORD:', w)\n",
    "    try:\n",
    "        for w, v in task2_wv.most_similar(w):\n",
    "            print('   ', w, v)\n",
    "    except:\n",
    "        print(f'{w} not found in the training set')\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "d711fdca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 3 (4 points)\n",
    "\n",
    "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
    "\n",
    "<pre>\n",
    "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "</pre>\n",
    "\n",
    "And this is its translation into Lower:\n",
    "\n",
    "<pre>\n",
    "the quick brown fox jumps over the lazy dog\n",
    "</pre>\n",
    "\n",
    "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
    "\n",
    "Prepare the corpora wich contains three kind of sentences:\n",
    "* Upper corpus sentences\n",
    "* Lower corpus sentences\n",
    "* sentences derived from Upper/Lower corpus, modified using D\n",
    "\n",
    "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
    "\n",
    "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
    "\n",
    "<pre>\n",
    "WOLF, CAT, WOLVES, LION, gopher, dog\n",
    "</pre>\n",
    "\n",
    "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.auto import tqdm\n",
    "from string import punctuation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "with open('task3_polish_lower.txt', 'r') as in_file:\n",
    "    lower_sentences = in_file.read()\n",
    "\n",
    "lower_sentences = lower_sentences.split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500001 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5c0f86800804865af42cb4a9aac8cfa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_polish_lower_clean.txt', 'w') as out_file:\n",
    "    for sentence in tqdm(lower_sentences):\n",
    "        clean_words = []\n",
    "        for word in sentence.split(' '):\n",
    "            if word.isnumeric():\n",
    "                clean_words.append('<num>')\n",
    "            elif word not in punctuation:\n",
    "                clean_words.append(word)\n",
    "        out_file.write(' '.join(clean_words) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500001 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a71f6f5f7394bc7be3e0ec92b1cdbda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_polish_upper.txt', 'r') as in_file:\n",
    "    upper_sentences = in_file.read()\n",
    "\n",
    "upper_sentences = upper_sentences.split('\\n')\n",
    "\n",
    "with open('task3_polish_upper_clean.txt', 'w') as out_file:\n",
    "    for sentence in tqdm(upper_sentences):\n",
    "        clean_words = []\n",
    "        for word in sentence.split(' '):\n",
    "            if word.isnumeric():\n",
    "                clean_words.append('<num>')\n",
    "            elif word not in punctuation:\n",
    "                clean_words.append(word)\n",
    "        out_file.write(' '.join(clean_words) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "with open('task3_polish_lower_clean.txt', 'r') as in_file:\n",
    "    lower_sentences = in_file.read()\n",
    "\n",
    "lower_sentences = lower_sentences.split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "lower_word_counts = pd.Series(lower_sentences).apply(lambda sentence: str(sentence).split(' ')).explode().value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task3_polish_lower_clean.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    kot_kota 0.7046511173248291\n",
      "    kot 0.700019896030426\n",
      "    koń 0.6674674153327942\n",
      "    dzieciak 0.6413710713386536\n",
      "    zwierzyć_zwierzę 0.6334779262542725\n",
      "    ubranie 0.6181987524032593\n",
      "    zwierzę 0.6084510684013367\n",
      "    chłopiec 0.6027811169624329\n",
      "    dziewczyna 0.6027806401252747\n",
      "    szczeniak 0.5959733128547668\n",
      "\n",
      "WORD: smok\n",
      "    szpon 0.6890965700149536\n",
      "    kontur 0.6756772398948669\n",
      "    błękitny 0.6708981394767761\n",
      "    kieł 0.6688380241394043\n",
      "    słoń 0.668688952922821\n",
      "    pięść 0.6665152311325073\n",
      "    wilk 0.665794312953949\n",
      "    młodzieniec 0.6567079424858093\n",
      "    tan 0.6550998091697693\n",
      "    płomień 0.6548458337783813\n",
      "\n",
      "WORD: miłość\n",
      "    bóg 0.7502952218055725\n",
      "    dobroć 0.7225184440612793\n",
      "    dusza 0.7164352536201477\n",
      "    zbawienie 0.692441999912262\n",
      "    życzliwość 0.6892829537391663\n",
      "    przyjaźnić_przyjaźń 0.6856911182403564\n",
      "    uczucie 0.6825954914093018\n",
      "    miłosierdzie 0.6780387163162231\n",
      "    modlitwa 0.6756280660629272\n",
      "    zmartwychwstanie_zmartwychwstać 0.6751652956008911\n",
      "\n",
      "WORD: rower\n",
      "    motocykl 0.7596251964569092\n",
      "    samochód 0.6822885870933533\n",
      "    motorower 0.6729142069816589\n",
      "    pieszy 0.6716234087944031\n",
      "    przejażdżka 0.6700480580329895\n",
      "    autobus 0.6645796298980713\n",
      "    narta 0.6496196985244751\n",
      "    wóz 0.6443701386451721\n",
      "    przyczepa 0.6375644207000732\n",
      "    podjazd 0.6368052363395691\n",
      "\n",
      "WORD: maraton\n",
      "    rajd 0.7414681315422058\n",
      "    uniwersjada 0.7285810112953186\n",
      "    gal_gala 0.7271028161048889\n",
      "    zmaganie 0.6955394744873047\n",
      "    olimpiada 0.695253312587738\n",
      "    wyścig 0.6935303807258606\n",
      "    regaty 0.6820915341377258\n",
      "    festiwal 0.6696428656578064\n",
      "    wyprawa 0.6653673052787781\n",
      "    mityng 0.6554850935935974\n",
      "\n",
      "WORD: logika\n",
      "    idea 0.537954568862915\n",
      "    filozofia 0.5276547074317932\n",
      "    ideał 0.5146461129188538\n",
      "    sens 0.49922606348991394\n",
      "    logicznie 0.4973422586917877\n",
      "    wola_woleć_woli 0.496890664100647\n",
      "    wizja 0.4956134855747223\n",
      "    koncepcja 0.48975327610969543\n",
      "    teza 0.48745858669281006\n",
      "    jasno 0.4873238205909729\n",
      "\n",
      "WORD: motyl\n",
      "    ziarnisty 0.7289221882820129\n",
      "    lecący 0.7239576578140259\n",
      "    spirit 0.7197985053062439\n",
      "    ricardo 0.7165886759757996\n",
      "    kisiel 0.7133253216743469\n",
      "    uzbecki 0.7119606733322144\n",
      "    rant 0.7108792662620544\n",
      "    bób 0.7092756628990173\n",
      "    lower 0.7042844295501709\n",
      "    friends 0.7040842771530151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task3_wv = model.wv\n",
    "\n",
    "example_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "for w in example_words:\n",
    "    print ('WORD:', w)\n",
    "    for w, v in task3_wv.most_similar(w):\n",
    "        print ('   ', w, v)\n",
    "    print ()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "lower_kmeans = KMeans(n_clusters=100)\n",
    "preds = lower_kmeans.fit_predict(task3_wv.vectors)\n",
    "\n",
    "preds_pdf = pd.DataFrame(preds, columns=['cluster'])\n",
    "preds_pdf.index = task3_wv.index_to_key\n",
    "preds_pdf = preds_pdf.join(lower_word_counts.rename('count'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "cluster_representatives_pdf = preds_pdf.groupby('cluster').apply(lambda pdf: pdf.sort_values('count', ascending=False).iloc[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 niemal\n",
      "0 stosunkowo\n",
      "0 nieuzyskania\n",
      "0 najprawdopodobniej\n",
      "0 twardy\n",
      "0 d\n",
      "0 przeważnie\n",
      "0 rzadziej\n",
      "0 ukryty\n",
      "0 równoległy\n",
      "1 częsty_część\n",
      "1 część\n",
      "1 głównie\n",
      "1 zielony\n",
      "1 górny\n",
      "1 leśny\n",
      "1 dolny\n",
      "1 górski\n",
      "1 położenie\n",
      "1 zasiąg_zasięg\n",
      "2 i\n",
      "2 postać\n",
      "2 jednakże\n",
      "2 trzeci_trzeć\n",
      "2 powstanie_powstać\n",
      "2 ewentualnie\n",
      "2 tzn.\n",
      "2 znalezienie\n",
      "2 ujęcie\n",
      "2 nawiązanie\n",
      "3 dotyczyć\n",
      "3 wynikać\n",
      "3 wymagać\n",
      "3 stanowić\n",
      "3 oznaczać\n",
      "3 przewidywać\n",
      "3 pozwolić\n",
      "3 stan_stanowić_stanowy\n",
      "3 spowodować\n",
      "3 nastąpić\n",
      "4 przyjęcie\n",
      "4 uzyskanie\n",
      "4 zakończenie\n",
      "4 wejście\n",
      "4 wykonanie\n",
      "4 podjęcie\n",
      "4 rozpatrzenie\n",
      "4 ustalenie\n",
      "4 wydanie\n",
      "4 przeprowadzenie\n",
      "5 <num>\n",
      "5 od\n",
      "5 pierwszy\n",
      "5 dwa\n",
      "5 jeden_jedny\n",
      "5 drugi\n",
      "5 kolejny\n",
      "5 ostatni\n",
      "5 trzy\n",
      "5 kilka\n",
      "6 projekt\n",
      "6 pytanie\n",
      "6 wniosek\n",
      "6 uwaga\n",
      "6 głos\n",
      "6 stanowisko\n",
      "6 odpowiedź\n",
      "6 propozycja\n",
      "6 opinia\n",
      "6 sprawozdanie\n",
      "7 plus\n",
      "7 in\n",
      "7 kalendarz\n",
      "7 agent\n",
      "7 e\n",
      "7 podsumowanie\n",
      "7 pl.\n",
      "7 podopieczny\n",
      "7 klucz\n",
      "7 sprzedający\n",
      "8 liczba\n",
      "8 poziom\n",
      "8 cena\n",
      "8 stosunek\n",
      "8 wzrost\n",
      "8 stopień\n",
      "8 wpływ\n",
      "8 wartość\n",
      "8 jakość\n",
      "8 ilość\n",
      "9 działanie\n",
      "9 działalność\n",
      "9 wybór\n",
      "9 dyskusja\n",
      "9 spotkanie\n",
      "9 współpraca\n",
      "9 akcja\n",
      "9 debata\n",
      "9 walka\n",
      "9 konkurs\n"
     ]
    }
   ],
   "source": [
    "for n, (cluster, word) in enumerate(cluster_representatives_pdf.index.values):\n",
    "    if n == 100:\n",
    "        break\n",
    "    print(cluster, word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "representatives = [idx[1] for idx in cluster_representatives_pdf.index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "with open('task3_representatives.txt', 'w') as out_file:\n",
    "    out_file.write(' '.join(representatives))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "repr_sentences = defaultdict(list)\n",
    "representatives = set(representatives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "for sentence in lower_sentences:\n",
    "    sentence_words = set(sentence.split(' '))\n",
    "    for word in list(sentence_words & representatives):\n",
    "        repr_sentences[word].append(sentence)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/996 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0b3bbe52c9e404897e6b5df133cc44f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_lower_to_upper.txt', 'w') as out_file:\n",
    "    for representative, sentences in tqdm(repr_sentences.items(), total=len(repr_sentences)):\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split(' ')\n",
    "            for idx, word in enumerate(sentence_words):\n",
    "                if word == representative:\n",
    "                    sentence_words[idx] = word.upper()\n",
    "            out_file.write(' '.join(sentence_words) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "with open('task3_polish_upper_clean.txt', 'r') as in_file:\n",
    "    upper_sentences = in_file.read()\n",
    "upper_sentences = upper_sentences.split('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "representatives_upper = set([representative.upper() for representative in list(representatives)])\n",
    "repr_sentences = defaultdict(list)\n",
    "\n",
    "for sentence in upper_sentences:\n",
    "    sentence_words = set(sentence.split(' '))\n",
    "    for word in list(sentence_words & representatives_upper):\n",
    "        repr_sentences[word].append(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/995 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4940da8db25d4ceb8b4b3d090d76d5d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('task3_upper_to_lower.txt', 'w') as out_file:\n",
    "    for representative, sentences in tqdm(repr_sentences.items(), total=len(repr_sentences)):\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split(' ')\n",
    "            for idx, word in enumerate(sentence_words):\n",
    "                if word == representative:\n",
    "                    sentence_words[idx] = word.lower()\n",
    "            out_file.write(' '.join(sentence_words) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "with open('task3_corpus.txt', 'w') as out_file:\n",
    "    for file in ['task3_polish_lower_clean.txt', 'task3_polish_upper_clean.txt', 'task3_lower_to_upper.txt', 'task3_upper_to_lower.txt']:\n",
    "        with open(file, 'r') as in_file:\n",
    "            shutil.copyfileobj(in_file, out_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='task3_corpus.txt', vector_size=100, min_count=1, workers=4, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "model.save('task3.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "upper_words = list(pd.Series(upper_sentences).apply(lambda sentence: str(sentence).split(' ')).explode().unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "upper_words = np.array([word for word in upper_words if len(word) > 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "representatives = list(representatives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def evaluate(word: str, model: Word2Vec):\n",
    "    distances = model.wv.distances(word, other_words=list(upper_words))\n",
    "    try:\n",
    "        position = np.argwhere(upper_words[np.argsort(distances)] == word.upper())[0][0]\n",
    "    except:\n",
    "        print(word)\n",
    "        return 0\n",
    "    return 1 / (position+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def most_similar(word: str, model: Word2Vec, n: int = 10):\n",
    "    distances = model.wv.distances(word, other_words=list(upper_words))\n",
    "    return upper_words[np.argsort(distances)][:n]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/996 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d270e3e00294085a55cb86ff4d8b1b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<num>\n",
      "0.8154163699959579\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for word in tqdm(representatives):\n",
    "    scores.append(evaluate(word, model))\n",
    "\n",
    "print(np.mean(scores))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "                         0                     1\n0                ustalenie   0.16666666666666666\n1       pozostać_pozostały  0.027777777777777776\n2                państwowy                 0.125\n3                     mieć                   1.0\n4                       do   0.00847457627118644\n..                     ...                   ...\n991                 liczyć                   1.0\n992              połączony                   1.0\n993  stan_stanowić_stanowy                   1.0\n994                  nadal                  0.25\n995                 niższy                   1.0\n\n[996 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ustalenie</td>\n      <td>0.16666666666666666</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pozostać_pozostały</td>\n      <td>0.027777777777777776</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>państwowy</td>\n      <td>0.125</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mieć</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>do</td>\n      <td>0.00847457627118644</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>991</th>\n      <td>liczyć</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>992</th>\n      <td>połączony</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>993</th>\n      <td>stan_stanowić_stanowy</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>nadal</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>niższy</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>996 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array([representatives, scores]).T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['POTĘŻNIEJSZY', '4.1', 'DOMESDAY', '0-1', 'DOCHOWAĆ', 'USTALENIE',\n       '15,5', 'WIADOMY', 'OCHŁODZENIE', 'FRANCESCO'], dtype='<U50')"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('ustalenie', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['ODMAWIAJĄC', 'LESS', 'FILED', 'WYŁOŻENIE', 'UNDER', 'TŁOKOWY',\n       'DZ.U.', 'PAŃSTWOWY', 'OŁAWA', 'PRZEWRACAĆ'], dtype='<U50')"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('państwowy', model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "lower_words = lower_word_counts.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "lower_words = np.array([word for word in lower_words if len(word) > 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4dd576583033481985255cc4eb9393da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "widzialem\n",
      "environment\n",
      "sagem\n",
      "erwin\n",
      "gigabyte\n",
      "tkacz\n",
      "before\n",
      "property\n",
      "maurice\n",
      "carla\n",
      "basidiomycota\n",
      "giuseppe\n",
      "powiedzial\n",
      "attack\n",
      "material\n",
      "czegos\n",
      "never\n",
      "vel\n",
      "tank\n",
      "0.10130278736314723\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for word in tqdm(np.random.choice(lower_words, 5000, replace=False)):\n",
    "    scores.append(evaluate(word, model))\n",
    "\n",
    "print(np.mean(scores))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "4947e307",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 4 (4 points)\n",
    "\n",
    "In this task you are asked to do two things:\n",
    "1. compare the embeddings computed on small corpus (like Brown Corpus , see: <https://en.wikipedia.org/wiki/Brown_Corpus>) with the ones coming from Google News Corpus\n",
    "2. Try to use other resourses like WordNet to enrich to corpus, and obtain better embeddings\n",
    "\n",
    "You can use the following code snippets:\n",
    "\n",
    "```python\n",
    "# printing tokenized Brown Corpora\n",
    "from nltk.corpus import brown\n",
    "for s in brown.sents():\n",
    "    print(*s)\n",
    "    \n",
    "#iterating over all synsets in WordNet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
    "    for synset in list(wn.all_synsets(synset_type)))[:10]:\n",
    "        print (synset.definition())\n",
    "        print (synset.examples())\n",
    "        print ([lem.name() for lem in synset.lemmas()])\n",
    "        print (synset.hyperonims()) # nodes 1 level up in ontology\n",
    "        \n",
    "# loading model and compute cosine similarity between words\n",
    "\n",
    "model = Word2Vec.load('models/w2v.wordnet5.model') \n",
    "print (model.wv.similarity('dog', 'cat'))\n",
    "```\n",
    "\n",
    "Embeddings will be tested using WordSim-353 dataset, the code showing the quality is in the cell below. Prepare the following corpora:\n",
    "1. Tokenized Brown Corpora\n",
    "2. Definitions and examples from Princeton WordNet\n",
    "3. (1) and (2) together\n",
    "4. (3) enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')\n",
    "\n",
    "Train 4 Word2Vec models, and raport Spearman correletion between similarities based on your vectors, and similarities based on human judgements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a2fbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code for computing correlation between W2V similarity, and human judgements\n",
    "\n",
    "import gensim.downloader\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "gn = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "for similarity_type in ['relatedness', 'similarity']:\n",
    "    ws353 = []\n",
    "    for x in open(f'wordsim_{similarity_type}_goldstandard.txt'): \n",
    "        a,b,val = x.split()\n",
    "        val = float(val)\n",
    "        ws353.append( (a,b,val))\n",
    "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
    "    print (similarity_type + ':', spearmanr(vals, ys)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf71c95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}